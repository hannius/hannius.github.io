<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[生成k8s证书的三种方式]]></title>
    <url>%2F2018%2F08%2F21%2F%E7%94%9F%E6%88%90k8s%E8%AF%81%E4%B9%A6%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[根据官方文档，生成k8s秘钥证书及相关管理证书有三种方式，其本质都是通过openssl: cfssl easyrsa openssl 官方文档：https://kubernetes.io/docs/concepts/cluster-administration/certificates/ cfssl方式1.cfssl下载地址:12345VERSION=R1.2for i in &#123;cfssl,cfssljson,cfssl-certinfo&#125;dowget https://pkg.cfssl.org/$&#123;VERSION&#125;/$&#123;i&#125;_linux-amd64 -O /usr/local/bin/$&#123;i&#125;done 2.生成CA配置文件1234567891011121314151617181920212223mkdir ssl &amp;&amp; cd sslcfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.jsoncat &gt; ca-config.json &lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOF 3.生成CA签名配置文件12345678910111213141516cat &gt; ca-csr.json &lt;&lt; EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names":[&#123; "C": "CN", "ST": "Beijing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125;]&#125;EOF 4.生成私钥和证书1cfssl gencert -initca ca-csr.json | cfssljson -bare ca 5.创建一个用于生成API Server的密钥和证书的JSON配置文件123456789101112131415161718192021222324252627cat &gt; kubernetes-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "hosts": [ "127.0.0.1", "&lt;MASTER_IP&gt;", "&lt;MASTER_CLUSTER_IP&gt;", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [&#123; "C": "CN", "ST": "Beijing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125;]&#125; EOF#该文件需要包含所有使用该证书的ip和域名列表，包括etcd集群、kubernetes master集群、以及apiserver 集群内部cluster ip。 6.生成 kubernetes 证书和私钥1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 7.创建admin证书12345678910111213141516171819202122cat &gt; admin-csr.json &lt;&lt;EOF&#123; "CN": "admin", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:masters", "OU": "System" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin# 证书O配置为system:masters 在集群内部cluster-admin的clusterrolebinding将system:masters组和cluster-admin clusterrole绑定在一起 8.创建kube-proxy证书123456789101112131415161718192021cat &gt; kube-proxy-csr.json &lt;&lt; EOF&#123; "CN": "system:kube-proxy", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy# 该证书用户名为system:kube-proxy，预定义的system:node-proxier clusterrolebindings将 system:kube-proxy用户和system:node-proxier角色绑定在一起 9.校验证书信息12cfssl-certinfo -cert kubernetes.pemopenssl x509 -noout -text -in kubernetes.pem 10.拷贝证书1mkdir -p /etc/kubernetes/ssl/ &amp;&amp; cp *.pem /etc/kubernetes/ssl/ easyrsa方式1.下载：12345curl -L -O https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gztar xzf easy-rsa.tar.gzcd easy-rsa-master/easyrsa3./easyrsa init-pki./easyrsa --batch "--req-cn=172.26.6.1@`date +%s`" build-ca nopass 2.生成 kubernetes 证书和私钥1./easyrsa --subject-alt-name="IP:172.26.6.1,IP:10.254.0.1,DNS:kubernetes.default" build-server-full kubernetes-master nopass 3.签发admin证书1./easyrsa --dn-mode=org --req-cn=admin --req-org=system:masters --req-c= --req-st= --req-city= --req-email= --req-ou= build-client-full admin nopass openssl方式1.生成ca12openssl genrsa -out ca.key 2048openssl req -x509 -new -nodes -key ca.key -subj "/CN=172.26.6.1" -days 10000 -out ca.crt 2.kubernetes证书和私钥12345678910111213141516171819202122232425262728293031323334353637383940openssl genrsa -out server.key 2048cat &gt;csr.conf &lt;&lt;EOF[ req ]default_bits = 2048prompt = nodefault_md = sha256req_extensions = req_extdistinguished_name = dn [ dn ]C = &lt;country&gt;ST = &lt;state&gt;L = &lt;city&gt;O = &lt;organization&gt;OU = &lt;organization unit&gt;CN = 172.26.6.1 [ req_ext ]subjectAltName = @alt_names [ alt_names ]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.clusterDNS.5 = kubernetes.default.svc.cluster.localIP.1 = 172.26.6.1IP.2 = 10.254.0.1 [ v3_ext ]authorityKeyIdentifier=keyid,issuer:alwaysbasicConstraints=CA:FALSEkeyUsage=keyEncipherment,dataEnciphermentextendedKeyUsage=serverAuth,clientAuthsubjectAltName=@alt_namesEOFopenssl req -new -key server.key -out server.csr -config csr.confopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 10000 -extensions v3_ext -extfile csr.confopenssl x509 -noout -text -in ./server.crt 3.admin证书123openssl genrsa -out admin.key 2048openssl req -new -key admin.key -out admin.csr -subj "/O=system:masters/CN=dmin"openssl x509 -req -set_serial $(date +%s%N) -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt -days 365 -extensions v3_req -extfile req.conf]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[只在移动端网页内显示”Fork me on Github”]]></title>
    <url>%2F2018%2F08%2F19%2F%E5%8F%AA%E5%9C%A8%E7%A7%BB%E5%8A%A8%E7%AB%AF%E7%BD%91%E9%A1%B5%E5%86%85%E6%98%BE%E7%A4%BA%E2%80%9DFork-me-on-Github%E2%80%9D%2F</url>
    <content type="text"><![CDATA[1.修改文件hexo博客根目录\themes\next\layout_layout.swig 找到如下代码块1234&lt;html class="&#123;&#123; html_class | lower &#125;&#125;" lang="&#123;&#123; config.language &#125;&#125;"&gt;&lt;head&gt; &#123;% include '_partials/head.swig' %&#125; &lt;title&gt;&#123;% block title %&#125;&#123;% endblock %&#125;&lt;/title&gt; 2.添加代码，结果如下：1234567891011121314151617&lt;html class="&#123;&#123; html_class | lower &#125;&#125;" lang="&#123;&#123; config.language &#125;&#125;"&gt;&lt;head&gt; &#123;% include '_partials/head.swig' %&#125; &lt;title&gt;&#123;% block title %&#125;&#123;% endblock %&#125;&lt;/title&gt; &#123;% include '_third-party/analytics/index.swig' %&#125; &lt;style&gt; .forkMeOnGithub&#123; display: none; &#125; @media (min-width: 768px) &#123; .forkMeOnGithub&#123; display: inline; &#125; &#125; &lt;/style&gt;&lt;/head&gt; 3.最后在之前引用代码块上套上div加上class就行了，代码如下123&lt;div class="forkMeOnGithub"&gt;&lt;a href="https://github.com/hannius"&gt;&lt;img style="position........&lt;/a&gt;&lt;/div&gt;]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes RBAC Detailed]]></title>
    <url>%2F2018%2F06%2F27%2Fbernetes-RBAC-Detailed%2F</url>
    <content type="text"><![CDATA[RBAC - 基于角色的访问控制RBAC使用：rbac.authorization.k8s.io API Group 来实现授权决策，允许管理员通过 Kubernetes API 动态配置策略，要启用RBAC，需要在 apiserver 中添加参数–authorization-mode=RBAC，如果使用的kubeadm安装的集群，1.6 版本以上的都默认开启了RBAC，可以通过查看 Master 节点上 apiserver 的静态Pod定义文件： 123456$ cat /usr/lib/systemd/system/kube-apiserver.service或者是：$ cat /etc/kubernetes/manifests/kube-apiserver.yaml... - --authorization-mode=Node,RBAC... 如果是二进制的方式搭建的集群，添加这个参数过后，记得要重启 apiserver 服务。 RBAC API 对象Kubernetes有一个很基本的特性就是它的所有资源对象都是模型化的 API 对象，允许执行 CRUD(Create、Read、Update、Delete)操作(也就是我们常说的增、删、改、查操作)，比如下面的这下资源： Pods ConfigMaps Deployments Nodes Secrets Namespaces 上面这些资源对象的可能存在的操作有： create get delete list update edit watch exec 在更上层，这些资源和API Group 进行关联，比如Pods属于Core API Group，而Deployements属于 apps API Group，要在Kubernetes中进行RBAC的管理，除了上面的这些资源和操作以外，我们还需要另外的一些对象： Rule：规则，规则是一组属于不同API Group 资源上的一组操作的集合 Role 和 ClusterRole：角色和集群角色，这两个对象都包含上面的Rules 元素，二者的区别在于，在Role 中，定义的规则只适用于单个命名空间，也就是和namespace 关联的，而ClusterRole 是集群范围内的，因此定义的规则不受命名空间的约束。另外Role和 ClusterRole在Kubernetes中都被定义为集群内部的API 资源，和Pod、ConfigMap 这些类似，都是集群的资源对象，所以同样的可以使用kubectl相关的命令来进行操作 Subject：主题，对应在集群中尝试操作的对象，集群中定义了3种类型的主题资源： User Account：用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用KeyStone或者Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的API 来进行管理 Group：组，这是用来关联多个账户的，集群中有一些默认创建的组，比如cluster-admin Service Account：服务帐号，通过Kubernetes API 来管理的一些用户帐号，和namespace 进行关联的，适用于集群内部运行的应用程序，需要通过API 来完成权限认证，所以在集群内部进行权限操作，都需要使用到 ServiceAccount RoleBinding和 ClusterRoleBinding：角色绑定和集群角色绑定，简单来说就是把声明的Subject和Role 进行绑定的过程(给某个用户绑定上操作的权限)，二者的区别也是作用范围的区别：RoleBinding只会影响到当前namespace 下面的资源操作权限，而ClusterRoleBinding会影响到所有的 namespace。 示例通过如下示例来演示RBAC的配置方法： 创建一个只能访问某个 namespace 的用户创建一个 User Account，只能访问 kube-system这个命名空间： username: martin group: op 第一步：创建用户凭证Kubernetes没有User Account的API 对象，不过要创建一个用户帐号的话也是挺简单的，利用管理员分配的一个私钥就可以创建了。创建方法有两种: 1. 使用OpenSSL证书来创建User； 给用户martin创建一个私钥，命名成：martin.key： 1$ openssl genrsa -out martin.key 2048 使用刚刚创建的私钥创建一个证书签名请求文件：martin.csr，要注意需要确保在-subj参数中指定用户名和组(CN表示用户名，O表示组)： 1$ openssl req -new -key martin.key -out martin.csr -subj "/CN=martin/O=op" 然后找到Kubernetes集群的CA，我们使用的是kubeadm安装的集群，CA相关证书位于/etc/kubernetes/pki/目录下面，如果是二进制方式搭建的，应该在最开始搭建集群的时候就已经指定好了CA的目录(/data/kubernetes/ssl)，然后利用该目录下面的ca.crt和ca.key两个文件来批准上面的证书请求 生成最终的证书文件，这里设置证书的有效期为500天 1234$ openssl x509 -req -in martin.csr -CA /data/kubernetes/ssl/ca.crt -CAkey /data/kubernetes/ssl/ca.key -CAcreateserial -out martin.crt -days 500现在查看当前文件夹下面是否生成了一个证书文件：$ lsmartin.csr martin.key martin.crt 现在可以使用刚刚创建的证书文件和私钥文件在集群中创建新的凭证和上下文(Context): 1$ kubectl config set-credentials martin --client-certificate=martin.crt --client-key=martin.key 可以看到一个用户martin创建了，然后为这个用户设置新的 Context: 1$ kubectl config set-context martin-context --cluster=kubernetes --namespace=kube-system --user=martin 到这里，用户martin就已经创建成功了，现在使用当前的这个配置文件来操作kubectl命令的时候，应该会出现错误，因为还没有为该用户定义任何操作的权限： 12$ kubectl get pods --context=martin-contextError from server (Forbidden): pods is forbidden: User "martin" cannot list pods in the namespace "default" 2. 使用cfssl工具来创建，也是参考官方文档中的方法。 CFSSL是CloudFlare开源的一款PKI/TLS工具。 CFSSL 包含一个命令行工具和一个用于签名，验证并且捆绑TLS证书的HTTP API服务。使用Go语言编写。 CFSSL包括： 一组用于生成自定义TLS PKI的工具 cfssl程序，是CFSSL的命令行工具 multirootca程序是可以使用多个签名密钥的证书颁发机构服务器 mkbundle程序用于构建证书池 cfssljson程序，从cfssl和multirootca程序获取JSON输出，并将证书，密钥，CSR和bundle写入磁盘 PKI借助数字证书和公钥加密技术提供可信任的网络身份。通常，证书就是一个包含如下身份信息的文件： 证书所有组织的信息 公钥 证书颁发组织的信息 证书颁发组织授予的权限，如证书有效期、适用的主机名、用途等 使用证书颁发组织私钥创建的数字签名 cfssl工具，子命令介绍： bundle: 创建包含客户端证书的证书包 genkey: 生成一个key(私钥)和CSR(证书签名请求) scan: 扫描主机问题 revoke: 吊销证书 certinfo: 输出给定证书的证书信息，跟cfssl-certinfo 工具作用一样 gencrl: 生成新的证书吊销列表 selfsign: 生成一个新的自签名密钥和签名证书 print-defaults: 打印默认配置，这个默认配置可以用作模板 serve: 启动一个HTTP API服务 gencert: 生成新的key(密钥)和签名证书 -ca：指明ca的证书 -ca-key：指明ca的私钥文件 -config：指明请求证书的json文件 -profile：与-config中的profile对应，是指根据config中的prof ile段来生成证书的相关信息 ocspdump ocspsign info: 获取有关远程签名者的信息 sign: 签名一个客户端证书，通过给定的CA和CA密钥，和主机名 ocsprefresh ocspserve 创建认证中心(CA)，也就是Kubernetes集群的CA，上面用openssl时已经将其省略了，现cfssl操作说明下详细方法： CFSSL可以创建一个获取和操作证书的内部认证中心。运行认证中心需要一个CA证书和相应的CA私钥。任何知道私钥的人都可以充当CA颁发证书。因此，私钥的保护至关重要。 创建用来生成CA文件的JSON配置文件,配置证书生成策略，让CA软件知道颁发什么样的证书。 12345678910111213141516171819202122232425[root@linux-node1 ssl]# vim ca-config.json&#123; "signing": &#123; "default": &#123; "expiry": "8760h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "8760h" &#125; &#125; &#125;&#125;这个策略，有一个默认的配置，和一个profile，可以设置多个profile，这里的profile 是kubernetes。默认策略，指定了证书的有效期是一年(8760h)kubernetes策略，指定了证书的用途signing, 表示该证书可用于签名其它证书；生成的ca.pem 证书中 CA=TRUEserver auth：表示client可以用该CA对server提供的证书进行验证client auth：表示server可以用该CA对client提供的证书进行验证 创建用来生成CA证书签名请求（CSR）的JSON配置文件 123456789101112131415161718192021222324[root@linux-node1 ssl]# vim ca-csr.json&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;术语介绍:CN: Common Name，浏览器使用该字段验证网站是否合法，一般写的是域名。非常重要。浏览器使用该字段验证网站是否合法C: Country， 国家L: Locality，地区，城市O: Organization Name，组织名称，公司名称OU: Organization Unit Name，组织单位名称，公司部门ST: State，州，省 生成CA证书（ca.pem）和私钥（ca-key.pem） 123456789[root@ linux-node1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca #初始化ca[root@ linux-node1 ssl]# ls -l ca*-rw-r--r-- 1 root root 290 Mar 4 13:45 ca-config.json-rw-r--r-- 1 root root 1001 Mar 4 14:09 ca.csr-rw-r--r-- 1 root root 208 Mar 4 13:51 ca-csr.json-rw------- 1 root root 1679 Mar 4 14:09 ca-key.pem-rw-r--r-- 1 root root 1359 Mar 4 14:09 ca.pem该命令会生成运行CA所必需的文件ca-key.pem（私钥）和ca.pem（证书），还会生成ca.csr（证书签名请求），用于交叉签名或重新签名。 小提示： 使用现有的CA私钥，重新生成： 1cfssl gencert -initca -ca-key key.pem ca-csr.json | cfssljson -bare ca 使用现有的CA私钥和CA证书，重新生成： 1cfssl gencert -renewca -ca cert.pem -ca-key key.pem 查看cert(证书信息)：cfssl certinfo -cert ca.pem 查看CSR(证书签名请求)信息：cfssl certinfo -csr ca.csr 创建martin证书签名请求(Kubernetes集群的CA创建好了，再根据该CA证书来创建一个只能访问某个namespace的用户) 12345678910111213141516171819202122➜ martin cat martin-csr.json &#123; "CN": "martin", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "op", #system:masters "OU": "System" &#125; ]&#125;➜ martin 后续kube-apiserver使用RBAC对客户端(如kubelet、kube-proxy、Pod)请求进行授权；kube-apiserver预定义了一些RBAC使用的RoleBindings，如cluster-admin将Group op(system:masters)与 Role cluster-admin 绑定，该Role授予了调用kube-apiserver的所有API的权限；OU指定该证书的Group为op(system:masters)，kubelet使用该证书访问 kube-apiserver时,由于证书被CA签名，所以认证通过，同时由于证书用户组为经过预授权的op(system:masters)，所以被授予访问所有 API 的权限； 生成martin证书和私钥 1234567891011121314151617181920➜ martin ls -lthtotal 4.0K-rw-r--r-- 1 root root 218 Jun 26 11:59 martin-csr.json➜ martin cfssl gencert -ca=/data/kubernetes/ssl/ca.pem -ca-key=/data/kubernetes/ssl/ca-key.pem -config=/data/kubernetes/ssl/ca-config.json -profile=kubernetes martin-csr.json|cfssljson -bare martin2018/06/26 16:20:37 [INFO] generate received request2018/06/26 16:20:37 [INFO] received CSR2018/06/26 16:20:37 [INFO] generating key: rsa-20482018/06/26 16:20:38 [INFO] encoded CSR2018/06/26 16:20:38 [INFO] signed certificate with serial number 4515304189457537416988994027390824160749108294022018/06/26 16:20:38 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 ("Information Requirements").➜ martin ls -lthtotal 16K-rw-r--r-- 1 root root 993 Jun 26 16:20 martin.csr-rw------- 1 root root 1.7K Jun 26 16:20 martin-key.pem-rw-r--r-- 1 root root 1.4K Jun 26 16:20 martin.pem-rw-r--r-- 1 root root 218 Jun 26 11:59 martin-csr.json➜ martin 设置集群参数 本段主要设置了需要访问的集群的信息。使用set-cluster设置了需要访问的集群，如下为kubernetes，这只是个名称，实际为–server指向的apiserver；–certificate-authority设置了该集群的公钥；–embed-certs为true表示将–certificate-authority证书写入到kubeconfig中；–server则表示该集群的kube-apiserver地址1234567891011121314151617181920212223➜ martin kubectl config set-cluster kubernetes --certificate-authority=/data/kubernetes/ssl/ca.pem --embed-certs=true --server=https://192.168.0.14:6443 --kubeconfig=martin.kubeconfigCluster "kubernetes" set.➜ martin ls -lthtotal 20K-rw------- 1 root root 2.0K Jun 26 16:29 martin.kubeconfig-rw-r--r-- 1 root root 993 Jun 26 16:20 martin.csr-rw------- 1 root root 1.7K Jun 26 16:20 martin-key.pem-rw-r--r-- 1 root root 1.4K Jun 26 16:20 martin.pem-rw-r--r-- 1 root root 218 Jun 26 11:59 martin-csr.json生成了新的文件：martin.kubeconfig➜ martin cat martin.kubeconfig apiVersion: v1clusters:- cluster: certificate-authority-data: xxx server: https://192.168.0.14:6443 name: kubernetescontexts: []current-context: ""kind: Configpreferences: &#123;&#125;users: []➜ martin 注意：–kubeconfig=martin.kubeconfig是将生成的相关信息全部写入martin.kubeconfig文件,如果不指定的话，默认是写入到“~/.kube/config ” 设置客户端认证参数 本段主要设置用户的相关信息，主要是用户证书。如下用户名为martin，证书为：/martin.pem，私钥为：./martin-key.pem。客户端的证书首先要经过集群CA的签署，否则不会被集群认可。此处使用的是ca认证方式，也可以使用token认证，如kubelet的TLS Boostrap机制下的bootstrapping使用的就是token认证方式。如下kubectl使用的是ca认证，不需要token字段 12345678➜ martin kubectl config set-credentials martin --client-certificate=./martin.pem --client-key=./martin-key.pem --embed-certs=true --kubeconfig=martin.kubeconfig User "martin" set.➜ martin 可以看到martin.kubeconfig新增了如下内容：users:- name: martin user: xxx 设置上下文参数,指定命名空间为：kube-system 集群参数和用户参数可以同时设置多对，在上下文参数中将集群参数和用户参数关联起来。下面的上下文名称为martin-context，集群为kubenetes，用户为martin，表示使用martin的用户凭证来访问kubenetes集群的kube-system命名空间(增加–namspace来指定访问的命名空间)。 执行之前先看下:martin.kubeconfig文件内容： 1234567891011121314151617➜ martin cat martin.kubeconfig apiVersion: v1clusters:- cluster: certificate-authority-data: xxx server: https://192.168.0.14:6443 name: kubernetescontexts: []current-context: ""kind: Configpreferences: &#123;&#125;users:- name: martin user: client-certificate-data: xxx client-key-data: xxx➜ martin 执行：123➜ martin kubectl config set-context martin-context --cluster=kubernetes --namespace=kube-system --user=martin --kubeconfig=martin.kubeconfig Context "martin-context" created.➜ martin 再次查看martin.kubeconfig文件,发现内容做了如下改变：之前：1contexts: [] 现在：123456contexts:- context: cluster: kubernetes namespace: kube-system user: martin name: martin-context 增加了上下文的相关信息。 设置默认上下文 12➜ martin kubectl config use-context martin-context --kubeconfig=martin.kubeconfigSwitched to context "martin-context". 如果配置了多个环境项，可以通过切换不同的环境项名字或指定kubeconfig文件来访问到不同的集群环境。 现在martin用户通过cfssl创建成功,可以看到所有关于martin用户的信息都写入了配置文件：martin.kubeconfig,不指定”-kubeconfig=”的话，默认是写入到”~/.kube/config”，如果之前有admin的相关信息，会追加到后面。martin.kubeconfig配置文件描述了集群、用户和上下文 kubectl只是个go编写的可执行程序，只要为kubectl配置合适的kubeconfig，就可以在集群中的任意节点使用。kubectl默认会从$HOME/.kube目录下查找文件名为config的文件，也可以通过设置环境变量KUBECONFIG或者通过设置–kubeconfig去指定其它kubeconfig文件,总之kubeconfig就是为访问集群所作的配置。 如果之前”~/.kube/config”下配置的是admin账号信息，要用martin账号，则指定kubeconfig文件即可： 12345678➜ martin kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE* kubernetes kubernetes admin ➜ martin ➜ martin kubectl config get-contexts --kubeconfig martin.kubeconfigCURRENT NAME CLUSTER AUTHINFO NAMESPACE* martin-context kubernetes martin kube-system➜ martin 现在使用当前的这个配置文件来操作kubectl命令的时候，应该会出现错误，因为还没有为该用户定义任何操作的权限呢：123➜ martin kubectl get pods --kubeconfig martin.kubeconfigError from server (Forbidden): pods is forbidden: User "martin" cannot list pods in the namespace "default"➜ martin 如果提示：“kubectl error: You must be logged in to the server (Unauthorized)”则是没有指定martin.kubeconfig文件或者默认的”~/.kube/config”里面没有martin用户的相关证书信息，因为前面设置客户端认证的时候没有指定password，而是用了证书。 1234kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword (用户名密码认证方式)kubectl config set-credentials martin --client-certificate=./martin.pem --client-key=./martin-key.pem --embed-certs=true --kubeconfig=martin.kubeconfig(证书认证方式) 另外可以通过：”Cfssl-Certinfo“命令来查看martin证书信息 12345678910111213141516171819202122232425262728293031323334➜ martin cfssl-certinfo -cert martin.pem&#123; "subject": &#123; "common_name": "martin", "country": "CN", "organization": "op", "organizational_unit": "System", "locality": "BeiJing", "province": "BeiJing", "names": [ "CN", "BeiJing", "BeiJing", "op", "System", "martin" ] &#125;, "issuer": &#123; "common_name": "kubernetes", "country": "CN", "organization": "k8s", "organizational_unit": "System", "locality": "BeiJing", "province": "BeiJing", "names": [ "CN", "BeiJing", "BeiJing", "k8s", "System", "kubernetes" ] &#125;, 第二步：创建角色用户创建完成后，接下来就需要给该用户添加操作权限，定义一个YAML文件，创建一个允许用户操作Deployment、Pod、ReplicaSets 的角色，如下定义：(martin-role.yaml) 1234567891011➜ martin cat martin-role.yaml apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: martin-role namespace: kube-systemrules:- apiGroups: ["", "extensions", "apps"] resources: ["deployments", "replicasets", "pods"] verbs: ["get", "list", "watch", "create", "update", "patch", "delete"] # 也可以使用['*']➜ martin 其中Pod属于core这个API Group，在YAML中用空字符就可以，而Deployment属于apps 这个API Group，ReplicaSets属于extensions这个API Group(点这里查文档)，所以 rules下面的apiGroups 就综合了这几个资源的 API Group：[“”, “extensions”, “apps”]，其中verbs就是上面提到的可以对这些资源对象执行的操作，这里需要所有的操作方法，所以也可以使用[‘*’]来代替。 然后创建这个Role: 123➜ martin kubectl create -f martin-role.yaml role.rbac.authorization.k8s.io "martin-role" created➜ martin 注意这里没有使用上面的martin-context这个上下文，因为木有权限 第三步：创建角色权限绑定Role创建完成了，但是很明显现在这个Role和我们的用户martin 还没有任何关系，这里就需要创建一个RoleBinding对象，在 kube-system这个命名空间下面将上面的martin-role角色和用户 martin进行绑定:(martin-rolebinding.yaml) 123456789101112131415161718➜ martin cat martin-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: martin-rolebinding namespace: kube-systemroleRef: #apiGroup: rbac.authorization.k8s.io #kind: ClusterRole kind: Role name: martin-role apiGroup: ""subjects:- apiGroup: "" kind: User name: martin #apiGroup: "" #会提示语法错误➜ martin 上面的YAML文件中看到了subjects关键字，这里就是上面提到的用来尝试操作集群的对象，这里对应上面的 User帐号martin，使用kubectl创建上面的资源对象： 12➜ martin kubectl create -f martin-rolebinding.yamlrolebinding.rbac.authorization.k8s.io "martin-rolebinding" created 使用admin账号(martin账号无权限查看)查看role和rolebinding相关信息： 12345678910111213141516171819202122232425➜ martin kubectl get rolebinding --all-namespacesNAMESPACE NAME AGEkube-public system:controller:bootstrap-signer 19dkube-system kubernetes-dashboard-minimal 11dkube-system martin-rolebinding 18hkube-system system::leader-locking-kube-controller-manager 19dkube-system system::leader-locking-kube-scheduler 19dkube-system system:controller:bootstrap-signer 19dkube-system system:controller:cloud-provider 19dkube-system system:controller:token-cleaner 19dkube-system ui-admin-binding 11dkube-system ui-read-binding 11d➜ martin ➜ martin kubectl get role --all-namespaces NAMESPACE NAME AGEkube-public system:controller:bootstrap-signer 19dkube-system extension-apiserver-authentication-reader 19dkube-system kubernetes-dashboard-minimal 11dkube-system martin-role 18hkube-system system::leader-locking-kube-controller-manager 19dkube-system system::leader-locking-kube-scheduler 19dkube-system system:controller:bootstrap-signer 19dkube-system system:controller:cloud-provider 19dkube-system system:controller:token-cleaner 19d➜ martin 第四步：测试现在应该可以用上面的martin-context上下文来操作集群了： 123456789101112131415161718➜ martin kubectl get pods NAME READY STATUS RESTARTS AGEnexus3-68f55d9746-vfnf8 1/1 Running 0 5drbd-rest-api-registrykey-m262-1 1/1 Running 0 4d➜ martin ➜ martin kubectl get pods -n defaultNAME READY STATUS RESTARTS AGEnexus3-68f55d9746-vfnf8 1/1 Running 0 5drbd-rest-api-registrykey-m262-1 1/1 Running 0 4d➜ martin ➜ martin kubectl get pods --kubeconfig martin.kubeconfigNAME READY STATUS RESTARTS AGEcoredns-77c989547b-lcbfw 1/1 Running 1 15dcoredns-77c989547b-xq4dr 1/1 Running 1 15dheapster-77b9c5bd7b-l5ms6 1/1 Running 0 11dkubernetes-dashboard-66c9d98865-g8l6l 1/1 Running 0 11dmonitoring-grafana-7c674cb7f6-nqvlw 1/1 Running 0 11dmonitoring-influxdb-644db5c5b6-llnp9 1/1 Running 0 11d 使用kubectl时并没有指定namespace，这是因为之前已经为该用户分配了权限，并且指定了kube-system命名空间写入到martin.kubeconfig文件中，如果使用default命名空间，在后面加上一个-n default，则会提示forbidden,如下：123➜ martin kubectl get pods -n default --kubeconfig martin.kubeconfig Error from server (Forbidden): pods is forbidden: User "martin" cannot list pods in the namespace "default"➜ martin 这是因为该用户并没有default这个命名空间的操作权限。 创建一个只能访问某个namespace的ServiceAccount上面创建了一个只能访问某个命名空间下面的普通用户，前面也提到过subjects,下面还有一种类型的主题资源：ServiceAccount。 第一步：创建一个集群内部的用户只能操作kube-system这个命名空间下面的pods和deployments首先来创建一个ServiceAccount对象： 123456789101112131415➜ serviceaccount kubectl create sa martin-sa -n kube-systemserviceaccount "martin-sa" created➜ serviceaccount ➜ serviceaccount kubectl get sa NAME SECRETS AGEdefault 1 20d➜ serviceaccount kubectl get sa -n kube-systemNAME SECRETS AGEadmin-user 1 11dcoredns 1 15ddefault 1 20dheapster 1 11dkubernetes-dashboard 1 11dmartin-sa 1 13s➜ serviceaccount 当然也可以定义成YAML文件的形式来创建: 12345apiVersion: v1kind: ServiceAccountmetadata: name: martin-sa namespace: kube-system 第二步：创建一个Role对象：(martin-sa-role.yaml)1234567891011121314➜ serviceaccount cat martin-sa-role.yaml apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: martin-sa-role namespace: kube-systemrules:- apiGroups: [""] resources: ["pods"] verbs: ["get", "list", "watch"] # 也可以使用['*']- apiGroups: ["apps"] resources: ["deployments"] verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]➜ serviceaccount 可以看到这里定义的角色没有创建、删除、更新Pod的权限，等会可以重点测试一下。 创建该Role对象：12➜ serviceaccount kubectl create -f martin-sa-role.yaml role.rbac.authorization.k8s.io "martin-sa-role" created 第三步，创建一个RoleBinding对象，将上面的martin-sa和角色martin-sa-role进行绑定：(martin-sa-rolebinding.yaml)1234567891011121314➜ serviceaccount cat martin-sa-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: martin-sa-rolebinding namespace: kube-systemsubjects:- kind: ServiceAccount name: martin-sa namespace: kube-systemroleRef: kind: Role name: martin-sa-role apiGroup: rbac.authorization.k8s.io 创建这个资源对象： 1234567891011121314151617➜ serviceaccount kubectl get rolebinding -n kube-systemNAME AGEkubernetes-dashboard-minimal 11dmartin-rolebinding 22h➜ serviceaccount ➜ serviceaccount kubectl create -f martin-sa-rolebinding.yaml rolebinding.rbac.authorization.k8s.io "martin-sa-rolebinding" created➜ serviceaccount ➜ serviceaccount kubectl get rolebinding No resources found.➜ serviceaccount kubectl get rolebinding -n kube-systemNAME AGEkubernetes-dashboard-minimal 11dmartin-rolebinding 23hmartin-sa-rolebinding 26s可以看到martin-sa-rolebinding已经添加 第四步，验证这个ServiceAccount一个ServiceAccount会生成一个Secret对象和它进行映射，这个Secret里面包含一个token：1234567891011121314151617181920212223242526➜ serviceaccount kubectl get secret -n kube-systemNAME TYPE DATA AGEadmin-user-token-xszp7 kubernetes.io/service-account-token 3 11dcoredns-token-9ppnq kubernetes.io/service-account-token 3 15ddefault-token-fs7zj kubernetes.io/service-account-token 3 20dheapster-token-gn8g5 kubernetes.io/service-account-token 3 11dkubernetes-dashboard-certs Opaque 0 11dkubernetes-dashboard-key-holder Opaque 2 15dkubernetes-dashboard-token-tg782 kubernetes.io/service-account-token 3 11dmartin-sa-token-78s5j kubernetes.io/service-account-token 3 41m #新增➜ serviceaccount ➜ serviceaccount kubectl describe secret martin-sa-token-78s5j -n kube-systemName: martin-sa-token-78s5jNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name=martin-sa kubernetes.io/service-account.uid=576ef41d-79e2-11e8-bede-5254004f2222Type: kubernetes.io/service-account-tokenData====ca.crt: 1359 bytesnamespace: 11 bytestoken: xxx➜ serviceaccount ==注意：查看时需要-n指定kube-system命名空间！== 然后可以利用这个token去登录Dashboard，就可以在Dashboard中来验证功能是否符合预期： 123➜ serviceaccount kubectl get secret martin-sa-token-78s5j -o jsonpath=&#123;.data.token&#125; -n kube-system |base64 -d #会生成一串很长的base64后的字符串xxxxxxxxxxxxxxxx➜ serviceaccount 使用这里的xxx token去Dashboard页面进行登录：会出现如下提示信息： 1234configmaps is forbidden: User "system:serviceaccount:kube-system:martin-sa" cannot list configmaps in the namespace "default"closewarningpersistentvolumeclaims is forbidden: User "system:serviceaccount:kube-system:martin-sa" cannot list persistentvolumeclaims in the namespace "default" 这是因为登录进来后默认跳转到default命名空间，但是却没有改空间的权限，因此需要切换到kube-system命名空间下面: 原来url:https://xxx/#!/deployment?namespace=default 修改为新url:https://xxx/#!/deployment?namespace=kube-system 可以看到能访问pod列表了，但是也会有一些其他额外的提示：events is forbidden: User “system:serviceaccount:kube-system:martin-sa” cannot list events in the namespace “kube-system”，这是因为当前登录用只被授权了访问pod和deployment的权限，同样的，访问下deployment看看可以了吗？ 同样的，可以根据自己的需求来对访问用户的权限进行限制，可以自己通过Role定义更加细粒度的权限，也可以使用系统内置的一些权限…… 创建一个可以访问所有 namespace 的ServiceAccount刚刚创建的martin-sa这个ServiceAccount和一个Role角色进行绑定的，如果现在创建一个新的ServiceAccount，需要他操作的权限作用于所有的namespace，这个时候就需要使用到ClusterRole 和 ClusterRoleBinding 这两种资源对象了。 第一步，同样，首先新建一个ServiceAcount对象：(martin-sa2.yaml)12345678910111213141516171819202122➜ serviceaccount cat martin-sa2.yaml apiVersion: v1kind: ServiceAccountmetadata: name: martin-sa2 namespace: kube-system➜ serviceaccount ➜ serviceaccount kubectl create -f martin-sa2.yaml serviceaccount "martin-sa2" created➜ serviceaccount kubectl get sa NAME SECRETS AGEdefault 1 20d➜ serviceaccount kubectl get sa -n kube-systemNAME SECRETS AGEadmin-user 1 12dcoredns 1 15ddefault 1 20dheapster 1 12dkubernetes-dashboard 1 12dmartin-sa 1 1hmartin-sa2 1 12s➜ serviceaccount 第二步，创建一个ClusterRoleBinding 对象(martin-clusterolebinding.yaml):1234567891011121314➜ serviceaccount cat martin-clusterolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: martin-sa2-clusterrolebindingsubjects:- kind: ServiceAccount name: martin-sa2 namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io➜ serviceaccount 对比下之前的”martin-sa-rolebinding.yaml” 123456789101112131415➜ serviceaccount cat martin-sa-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: martin-sa-rolebinding namespace: kube-systemsubjects:- kind: ServiceAccount name: martin-sa namespace: kube-systemroleRef: kind: Role name: martin-sa-role apiGroup: rbac.authorization.k8s.io➜ serviceaccount 从上面可以看到没有为这个资源对象声明namespace，因为这是一个ClusterRoleBinding 资源对象，是作用于整个集群的，也没有单独新建一个ClusterRole对象，而是使用的 cluster-admin这个对象，这是Kubernetes集群内置的ClusterRole对象，可以使用kubectl get clusterrole 和kubectl get clusterrolebinding查看系统内置的一些集群角色和集群角色绑定，这里使用的 cluster-admin这个集群角色是拥有最高权限的集群角色，所以一般需要谨慎使用该集群角色。 创建上面集群角色绑定资源对象： 123➜ serviceaccount kubectl create -f martin-clusterolebinding.yaml clusterrolebinding.rbac.authorization.k8s.io "martin-sa2-clusterrolebinding" created➜ serviceaccount 通过ubectl get clusterrolebinding可以看到”martin-sa2-clusterrolebinding”已经加入其中：1234567➜ serviceaccount kubectl get clusterrolebinding NAME AGEadmin-user 12dcluster-admin 20dheapster 12dkubelet-bootstrap 19dmartin-sa2-clusterrolebinding 22s 第三步，使用 ServiceAccount对应的token去登录Dashboard验证：1234567➜ serviceaccount kubectl get secret -n kube-system|grep martin-sa2-token-q7bhrmartin-sa2-token-q7bhr kubernetes.io/service-account-token 3 34m➜ serviceaccount ➜ serviceaccount kubectl get secret martin-sa2-token-q7bhr -o jsonpath=&#123;.data.token&#125; -n kube-system |base64 -dxxxxxxx#会生成一串很长的base64后的字符串➜ serviceaccount 在最开始接触到RBAC认证的时候，可能不太熟悉，特别是不知道应该怎么去编写rules规则，可以去分析系统自带的clusterrole、clusterrolebinding这些资源对象的编写方法，利用 kubectl的get、describe、-o yaml这些操作，所以kubectl最基本的操作一定要掌握好。 RBAC只是Kubernetes中安全认证的一种方式，当然也是现在最重要的一种方式。]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于k8s 动态配置及扩容maven nexus私服]]></title>
    <url>%2F2018%2F06%2F22%2F%E5%9F%BA%E4%BA%8Ek8s%20%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AE%E5%8F%8A%E6%89%A9%E5%AE%B9maven%20nexus%E7%A7%81%E6%9C%8D%2F</url>
    <content type="text"><![CDATA[配置nexus从官网下载了nexus之后还需要进行一些配置。编辑bin/nexus.vmoptions 调整后的如下： 12345678910111213141516-Xms600M-Xmx600M-XX:MaxDirectMemorySize=1G-XX:+UnlockDiagnosticVMOptions-XX:+UnsyncloadClass-XX:+LogVMOutput-XX:LogFile=/data/docker/soft/nexus/log/jvm.log-XX:-OmitStackTraceInFastThrow-Djava.net.preferIPv4Stack=true-Dkaraf.home=.-Dkaraf.base=.-Dkaraf.etc=etc/karaf-Djava.util.logging.config.file=etc/karaf/java.util.logging.properties-Dkaraf.data=/data/docker/soft/nexus/data-Djava.io.tmpdir=/data/docker/soft/nexus/tmp-Dkaraf.startLocalConsole=false 其中除了1，2行的jvm内存配置之外，最关键的就是，以下几个属性配置： -XX:LogFile=/data/docker/soft/nexus/log/jvm.log # 日志文件生成位置 -Dkaraf.data=/data/docker/soft/nexus/data # 仓库数据存放位置(上传的jar包) -Djava.io.tmpdir=/data/docker/soft/nexus/tmp # 临时文件存放位置 制作Docker镜像配置好nexus之后，需要再制作自己的docker镜像，因为k8s就是调度镜像容器的。 123456789[root@master nexus]# pwd/data/docker/dockerfile/nexus[root@master nexus]# ls -lth total 223M-rw-r--r-- 1 root root 146 Jun 21 16:06 Dockerfile-rw-r--r-- 1 root root 108M Jun 21 16:02 nexus3.tar.gzdrwxr-xr-x 3 root root 4.0K Jun 21 15:53 sonatype-workdrwxr-xr-x 9 root root 4.0K Jun 21 15:53 nexus-3.12.1-01-rw-r--r-- 1 root root 115M Jun 21 15:36 nexus-3.12.1-01-unix.tar.gz.org docker镜像的制作很简单，新建一个Dockerfile文件： 123456[root@master nexus]# cat Dockerfile FROM registry.cn-hangzhou.aliyuncs.com/luhaoyuan/oracle-jdk8:latestADD nexus3.tar.gz /optENTRYPOINT ["/opt/nexus-3.12.1-01/bin/nexus", "run"] 第一行：nexus的运行是依赖JDK环境的，所以我们这里就使用jdk作为基础镜像；(镜像是基于centos7，比较大，后续可以考虑修改为alpine_3.6) 第二行：将我们配置过后的nexus(nexus-3.12.1-01)再重新打包一下，添加到容器中； 第三行：启动容器时，执行的命令，nexus的启动命令有start和run，由于start默认是启动在后台进程的，这样容器一启动就退出了。所以这里必须要使用run命令启动了。 最后构建Docker镜像：docker build -t registry.martin.com:5000/tools/nexus:3.12.1 .registry.martin.com:5000为我registry地址,构建之后将改image push到私库,当然也可以用harbor如果有做ca校验，需要将证书拷贝到指定的:/etc/docker/certs.d/xxx/ca.crt,然后docker login校验再docker push registry.martin.com:5000/tools/nexus:3.12.1，不然会提示x509认证失败 配置k8s PV-PVC为了避免容器重启数据丢失，需要挂载主机的卷空间。在k8s中，pod挂载主机的存储卷，就需要使用到了PV（PersistentVolume）和PVC（PersistentVolumeClaim）。新建nexus3-pv-pvc.yaml文件： 12345678910111213141516171819202122232425262728293031323334353637383940[root@master nexus]# pwd/data/k8s/nexus[root@master nexus]# ls -lthtotal 12K-rw-r--r-- 1 root root 777 Jun 21 18:49 nexus3-deployment.yaml-rw-r--r-- 1 root root 370 Jun 21 17:12 nexus3-service.yaml-rw-r--r-- 1 root root 525 Jun 21 16:49 nexus3-pv-pvc.yaml[root@master nexus]# cat nexus3-pv-pvc.yaml apiVersion: v1kind: PersistentVolumemetadata: name: nexus3-data-pv labels: app: nexus3-data-pvspec: capacity: storage: 500Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle hostPath: path: /data/docker/soft/nexus---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nexus3-data-pvc labels: app: nexus3-data-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 500Gi selector: matchLabels: app: nexus3-data-pv[root@master nexus]# ==注意：PV中的hostPath，指定了宿主主机上的挂载路径(node节点最好全部先创建好)== 配置k8s Deployment在k8s早期更多的是使用ReplicationController (RC)来控制保障pod，不过后来又出现了Deployment。Deployment不仅包含了RC的所有功能，还具有：版本记录、回滚、暂停和启动等多种额外的强大功能。所以可以尽量都使用Deployment,新建nexus3-deployment.yaml文件： 123456789101112131415161718192021222324252627282930313233[root@master nexus]# cat nexus3-deployment.yaml kind: DeploymentapiVersion: extensions/v1beta1metadata: labels: app: nexus3 name: nexus3spec: replicas: 1 selector: matchLabels: app: nexus3 template: metadata: labels: app: nexus3 spec: containers: - name: nexus3 image: registry.martin.com:5000/tools/nexus:3.12.1 imagePullPolicy: IfNotPresent ports: - containerPort: 9193 protocol: TCP volumeMounts: - name: nexus-data mountPath: /data/docker/soft/nexus volumes: - name: nexus-data persistentVolumeClaim: claimName: nexus3-data-pvc nodeSelector: kubernetes.io/hostname: 192.168.0.15 需要在volumes结点上引用之前创建的PVC： 1234volumes: - name: nexus-data persistentVolumeClaim: claimName: nexus3-data-pvc 在volumeMounts结点上，配置了挂载到容器中的路径：/data/docker/soft/nexus(node节点最好全部先创建好) 123volumeMounts: - name: nexus-data mountPath: /data/docker/soft/nexus 最后的nodeSelector表示pod只在某个主机上运行,可以通过在k8s的master上使用:kubectl get nodes查看 配置k8s Servicek8s中的pod的访问是不可靠的，随时可能发生pod停止-漂移-创建的过程。所以要想能够稳定的访问，就必须要创建Service进行服务发现了，在Service中是根据selector来寻找pod的。最后k8s上的Service只能在集群节点上访问，如果我们想要在集群外部进行访问的话，只有三种方式： NodePort、 LoadBalancer、 Ingress。 这里使用NodePort，绑定宿主机的端口来进行暴露服务。跟docker run -p 看上去效果相似。新建nexus3-service.yaml文件：1234567891011121314151617[root@master nexus]# cat nexus3-service.yaml apiVersion: v1kind: Servicemetadata: labels: app: nexus3 name: nexus3spec: type: NodePort ports: - port: 8081 targetPort: 8081 nodePort: 30031 name: web-ui selector: app: nexus3[root@master nexus]# 其中关键的地方就是spec.type节点配置NodePort类型了。说明下的ports 端口的配置： port 属性定义了Service的虚端口； targetPort 属性指定了后面pod上提供的端口，如果没有指定则默认与port相同(这里我们显视的指定了)； nodePort 属性指定了绑定在宿主机(物理机)上的端口号，我们可以通过宿主机IP + 端口的形式访问到后方pod中的服。 name 如果有多个port配置的话，必须要为每个port指定一个名称。 k8s部署访问创建 PV-PVC根据配置文件，创建PV-PVC： kubectl create -f nexus3-pv-pvc.yaml 创建完成后，查看一下状态，是否正常： kubectl get pv 12NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnexus3-data-pv 500Gi RWO Recycle Bound default/nexus3-data-pvc 17h 创建Deployment继续创建Deployment，创建完后会自动创建pod的，并维护pod数量始终为1。 kubectl create -f nexus3-deployment.yaml 稍等几秒钟，查看pod状态： kubectl get pod -o wide 12NAME READY STATUS RESTARTS AGE IP NODEnexus3-68f55d9746-vfnf8 1/1 Running 0 12h 10.20.7.12 192.168.0.15 ==注意：默认不用-n指定namespace的都是用的default，-o wide可以看到详细的IP及node信息== 创建Service创建Service，暴露服务： 1kubectl create -f nexus3-service.yaml 查看状态： 12345 kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.10.0.1 &lt;none&gt; 443/TCP 13dnexus3 NodePort 10.10.165.3 &lt;none&gt; 8081:30031/TCP,5000:30032/TCP,8889:30033/TCP 12hnginx-service ClusterIP 10.10.147.216 &lt;none&gt; 80/TCP 10d ==注意这里的访问，是访问宿主机的IP+端口，至于CLUSTER-IP这些都是虚拟的IP，无法在外部进行访问的==。 访问Nexushttp://192.168.1.2:30031 (pod内container端口为：8081) 升级借用k8s Deployment的升级方式: 从官网下载最新的nexus安装包； 修改nexus配置文件，将上面旧版本的配置覆盖过来就行了； 修改Dockerfile文件，构建新的Docker镜像，将新打包的nexus放入镜像中。如：docker build -t registry.martin.com:5000/tools/nexus:3.12.2 .Ps: 不要忘记启动命令路径也要调整! 使用k8s命令升级Deployment：如：kubectl set image deployment/nexus3 nexus3=registry.martin.com:5000/tools/nexus:3.12.2 回滚升级，如果发现升级了的不好用，或者出现问题，也可以回滚：如：kubectl rollout undo deployment/nexus3 总结其实这个过程中里复杂了一部分，也简化了一部分。复杂了pv-pvc过程，pv-pvc不用创建直接在Deployment中挂载hostPath也是可以的。简化了Deployment，其实应该还需要加上cpu、内存等资源限制的。这里只是在nexus配置文件中做了限制，如果出现内存泄漏问题，还是没办法解决!]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes-jenkins-ci-cd]]></title>
    <url>%2F2018%2F06%2F14%2Fkubernetes-jenkins-ci-cd%2F</url>
    <content type="text"><![CDATA[流程图：基于Jenkins的CI/CD流程如下所示: 流程说明： 用户向Gitlab提交代码，代码中必须包含Dockerfile 将代码提交到远程仓库 用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发Jenkins自动构建 Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库 Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的kubernetes的YAML模板，将其中的变量替换成用户输入的选项 生成应用的kubernetes YAML配置文件 更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息 更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看边缘节点配置 Jenkins调用kubernetes的API，部署应用]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pause容器]]></title>
    <url>%2F2018%2F06%2F13%2FPause%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Pause容器定义Pause容器，又叫Infra容器，本文将探究该容器的作用与原理。 在kubelet的配置中有这样一个参数： 1KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是openshift中的配置参数，kubernetes中默认的配置参数是： 1KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause容器，是可以自己来定义，官方使用的gcr.io/google_containers/pause-amd64:3.0容器的代码见Github，使用C语言编写。 Pause容器的作用检查nod节点的时候会发现每个node上都运行了很多的pause容器，例如如下: 12345678[root@elk-02 bin]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES576c56bd6065 mirrorgooglecontainers/kubernetes-dashboard-amd64 "/dashboard --inse..." 2 hours ago Up 2 hours k8s_kubernetes-dashboard_kubernetes-dashboard-66c9d98865-jdbg8_kube-system_d2406f4f-6de3-11e8-8760-5254004f2222_0c4985381c2b7 d4b7466213fe "/coredns -conf /e..." 2 hours ago Up 2 hours k8s_coredns_coredns-77c989547b-xq4dr_kube-system_d23ef2c4-6de3-11e8-8760-5254004f2222_1ba2fef1cbf00 mirrorgooglecontainers/pause-amd64:3.0 "/pause" 2 hours ago Up 2 hours k8s_POD_coredns-77c989547b-xq4dr_kube-system_d23ef2c4-6de3-11e8-8760-5254004f2222_1ea6c2994b397 d4b7466213fe "/coredns -conf /e..." 2 hours ago Up 2 hours k8s_coredns_coredns-77c989547b-lcbfw_kube-system_0696926b-6d79-11e8-8760-5254004f2222_1f61476c51230 mirrorgooglecontainers/pause-amd64:3.0 "/pause" 2 hours ago Up 2 hours k8s_POD_kubernetes-dashboard-66c9d98865-jdbg8_kube-system_d2406f4f-6de3-11e8-8760-5254004f2222_0b6f61200d5ea mirrorgooglecontainers/pause-amd64:3.0 "/pause" 2 hours ago Up 2 hours k8s_POD_coredns-77c989547b-lcbfw_kube-system_0696926b-6d79-11e8-8760-5254004f2222_1 kubernetes中的pause容器主要为每个业务容器提供以下功能： 在pod中担任Linux命名空间共享的基础； 启用pid命名空间，开启init进程。 pause容器的作用可以从这个例子中看出，首先见下图： Pause容器测试首先在节点上运行一个pause容器。 1docker run -d --name pause -p 8880:80 martin/pause-amd64:3.0 然后再运行一个nginx容器，nginx将为localhost:2398创建一个代理。 123456789101112131415$ cat &lt;&lt;EOF &gt;&gt; nginx.confferror_log stderr;events &#123; worker_connections 1024; &#125;http &#123; access_log /dev/stdout combined; server &#123; listen 80 default_server; server_name example.com www.example.com; location / &#123; proxy_pass http://127.0.0.1:2398; &#125; &#125;&#125;EOF$ docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf --net=container:pause --ipc=container:pause --pid=container:pause nginx 然后再为ghost创建一个应用容器，这是一款博客软件。 1$ docker run -d --name ghost --net=container:pause --ipc=container:pause --pid=container:pause ghost 现在访问http://localhost:8880/就可以看到ghost博客的界面了。 Pause容器解析pause容器将内部的80端口映射到宿主机的8880端口，pause容器在宿主机上设置好了网络namespace后，nginx容器加入到该网络namespace中，我们看到nginx容器启动的时候指定了–net=container:pause，ghost容器同样加入到了该网络namespace中，这样三个容器就共享了网络，互相之间就可以使用localhost直接通信，–ipc=contianer:pause –pid=container:pause就是三个容器处于同一个namespace中，init进程为pause，这时我们进入到ghost容器中查看进程情况。 12345678# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 1024 4 ? Ss 13:49 0:00 /pauseroot 5 0.0 0.1 32432 5736 ? Ss 13:51 0:00 nginx: master psystemd+ 9 0.0 0.0 32980 3304 ? S 13:51 0:00 nginx: worker pnode 10 0.3 2.0 1254200 83788 ? Ssl 13:53 0:03 node current/inroot 79 0.1 0.0 4336 812 pts/0 Ss 14:09 0:00 shroot 87 0.0 0.0 17500 2080 pts/0 R+ 14:10 0:00 ps aux 在ghost容器中同时可以看到pause和nginx容器的进程，并且pause容器的PID是1。而在kubernetes中容器的PID=1的进程即为容器本身的业务进程。 参考Kubernetes只Pause容器 kubernetes中的infra容器——Pause容器探究]]></content>
      <categories>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab、anacron、logrotate relationship]]></title>
    <url>%2F2018%2F05%2F24%2Fcrontab%E5%92%8Canacron%E5%92%8Clogrotate-relation%2F</url>
    <content type="text"><![CDATA[服务器上的nginx使用logrotate来分割日志，设置为每天分割。但是logrotate似乎没有工作，日志并没有分割。服务器是CentOS 6。 为了找到原因，分析可能出错的地方。如果是logrotate未执行，可能是crond没有启动，因为logrotate被/etc/cron.daily/logrotate脚本所启动，可以查看其中代码： 123456789[root@test ~]# cat /etc/cron.daily/logrotate#!/bin/sh/usr/sbin/logrotate /etc/logrotate.confEXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"fiexit 0 可以看到logrotate运行时加载配置文件logrotate.conf，而这个配置文件除了设定一些分割日志相关的选项，还包含分割日志的配置文件目录/etc/logrotate.d。 nginx的日志分割配置文件就保存在logrotate.d目录： 12345678910111213141516[root@test ~]# cat !$cat /etc/logrotate.d/nginx/root/*.log &#123; Daily Missingok rotate 52 compress delaycompress notifempty dateext create 644 nobody nobody sharedscripts postrotate [ -f /usr/local/nginx/logs/nginx.pid ] &amp;&amp; kill -USR1 `cat /usr/local/nginx/logs/nginx.pid` endscript&#125; /root/*.log就是需要被分割的日志的目录，通配符*表示目录内的所有log文件都被分割，分割的规则就是{…}中的内容。这里把/root/*.log当做nginx日志只是为了测试。在启动crond服务后，发现日志还是没有分割，于是想到会不会是/etc/logrotate.d/nginx配置文件的语法有问题，使用以下命令调试这个文件： 1logrotate -vfd /etc/logrotate.d/nginx # -vfd 三个选项分别表示显示详情，强制分割日志，只是调试配置文件而不是真的分割日志 输出结果表明有语法错误，Daily，Missingok 都应该是小写。改成daily，missingok。再次调试配置文件，可以正确分割日志： 123[root@test ~]# ls -1 /root/install-2017-5-14.loginstall-2017-5-14.log-20170521 #logrotate归档的日志 上面猜测是crond执行/etc/cron.daily/内的脚本，实现定时执行计划任务，包括执行logrotate日志分割。为了验证是否正确，网上搜索一番后找到了答案。如果没有crontab命令，先安装： 1234yum install crontabs #安装crond，crond实际上来自cronie包，这个包作为crontabs包的依赖被安装chkconfig --add crond #添加到开机启动列表chkconfig crond on #开机启动crond服务/etc/init.d/crond #立即启动crond 以下文件或目录的作用：cron计划任务有两种类型： 1）系统cron任务：由crond服务执行，/etc/crontab配置系统级别的任务 2）用户cron任务：由crond服务执行，用crontab命令编辑用户级别的任务 属于系统cron任务的文件或目录： /etc/cron.d #系统的任务脚本。执行 rpm -ql cronie 可以看到该目录被cronie包安装 /etc/cron.hourly #每小时执行其内脚本。其中的0anacron文件调用anacron来执行任务，它被包cronie-anacron安装 /etc/cron.daily #每天执行其内脚本。也被anacron执行其内脚本，logrotate调用脚本就在该目录内 /etc/cron.weekly #每周执行其内脚本。 /etc/cron.monthly #每月执行其内脚本。 控制用户cron任务的执行： /etc/cron.allow #默认不存在，如果这个文件存在，只有用户在这个文件中才能使用crontab命令 /etc/cron.deny #将不可以使用crontab命令的用户写入其中 注意：cron.allow和cron.deny就是用户名的列表，每行一个用户名。比如 cron.deny中有一行jason，效果是如果当前登录用户是jason，执行 crontab -e会提示不允许使用crontab命令。 以下三个目录的作用： /var/spool/cron/USER_NAME #这个文件才是跟crontab -e/-l 关联的，这个文件保存了crontab -e编辑的任务内容 #比如执行 crontab -u root -e，编辑保存后，就会有/var/spool/cron/root 这个文件 /var/spool/anacron/{cron.daily,cron.monthly,cron.weekly} #这三个文件记录了anacron上一次执行的时间（上一天，上一周或上一月） #anacron任务执行时，对照这里的时间，决定是否执行anacron任务 /var/lib/logrotate.status #这个文件记录logrotate执行情况，logrotate参考这个文件来决定是否需要rotate日志 crontab和anacron和logrotate的关系： 123456[root@test ~]# cat /etc/cron.d/0hourly #这个文件指定每小时的01分执行/etc/cron.hourly内的所有脚本SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=rootHOME=/01 * * * * root run-parts /etc/cron.hourly #这里的root指定执行任务的用户，run-parts其实是一个可执行脚本，在/usr/bin/run-parts，用来执行cron.hourly目录内的所有脚本 说明：用crontab -e命令每次编辑完某个用户的cron设置后，cron自动在/var/spool/cron下生成一个与此用户同名的文件，此用户的cron信息都记录在这个文件中。cron启动后每过一份钟读一次这个文件，检查是否要执行里面的命令。因此此文件修改后不需要重新启动cron服务。cron服务每分钟不仅要读一次/var/spool/cron内的所有文件，还需要读一次/etc/crontab，因此我们配置这个文件也能运用cron服务做一些事情。用crontab命令配置是针对某个用户的，而编辑/etc/crontab是针对系统的任务。此文件的文件格式是： 1234SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/bin #可执行文件查找路径MAILTO=root #如果出现错误，或者有数据输出，数据作为邮件发给这个帐号HOME=/ #使用者运行的路径，这里是根目录 123456789101112131415161718[root@test ~]# cat /etc/cron.hourly/0anacron #cron.hourly目录下的脚本，根据条件执行anacron命令#!/bin/bash# Skip excecution unless the date has changed from the previous runif test -r /var/spool/anacron/cron.daily; then day=`cat /var/spool/anacron/cron.daily`fiif [ `date +%Y%m%d` = "$day" ]; then exit 0;fi# Skip excecution unless AC poweredif test -x /usr/bin/on_ac_power; then /usr/bin/on_ac_power &amp;&gt; /dev/null if test $? -eq 1; then exit 0 fifi/usr/sbin/anacron -s 1234567891011121314151617[root@test ~]# cat /etc/anacrontab #如果执行anacron命令，那么接着查看anacron的配置文件# /etc/anacrontab: configuration file for anacron# See anacron(8) and anacrontab(5) for details.SHELL=/bin/shPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# the maximal random delay added to the base delay of the jobsRANDOM_DELAY=45 #最大延迟时间# the jobs will be started during the following hours onlySTART_HOURS_RANGE=3-22 #只有在3-22点之间执行任务#period in days delay in minutes job-identifier command1 5 cron.daily nice run-parts /etc/cron.daily7 25 cron.weekly nice run-parts /etc/cron.weekly@monthly 45 cron.monthly nice run-parts /etc/cron.monthly 以上anacrontab配置文件最重要的是最后一部分，以这行为例： 1 5 cron.daily nice run-parts /etc/cron.daily 表示每天都执行/etc/cront.daily/目录下的脚本文件，真实的延迟是RANDOM_DELAY+delay。这里的延迟是5分钟，加上上面的RANDOM_DELAY，所以实际的延迟时间是5-50之间，开始时间为03-22点，如果机器没关，那么一般就是在03:05-03:50之间执行。nice命令将该进程设置为nice=10，默认为0，即低优先级进程。如果RANDOM_DELAY=0，那么表示准确延迟5min，即03:05执行cron.daily内的脚本。 123456789[root@test ~]# cat /etc/cron.daily/logrotate #最后在cron.daily内有logrotate的调用脚本#!/bin/sh/usr/sbin/logrotate /etc/logrotate.conf #logrotate将会读取配置文件，最终会读取到/etc/logrotate.d/nginxEXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"fiexit 0 当logrotate命令加载了/etc/logrotate.d/nginx配置文件时，还要比较nginx日志的归档日期： 12[root@test ~]# cat /var/lib/logrotate.status | grep /root"/root/install-2017-5-14.log" 2017-5-21 #如果今天是2017-5-21，这个文件里也是2017-5-21，说明今天已经归档过了，否则就会归档（分割）nginx日志 综上，整个逻辑流程为： crond服务加载/etc/cron.d/0hourly —&gt;在每小时的01分执行/etc/cront.hourly/0anacron —&gt;执行anacron —&gt;根据/etc/anacrontab的配置执行/etc/cron.daily，/etc/cron.weekly，/etc/cron.monthly —&gt;执行/etc/cron.daily/下的logrotate脚本 —&gt;执行logrotate —&gt;根据/etc/logrotate.conf配置执行脚本/etc/logrotate.d/nginx —&gt;分割nginx日志成功]]></content>
      <categories>
        <category>Logrotate</category>
      </categories>
      <tags>
        <tag>Logrotate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK Sentinl]]></title>
    <url>%2F2018%2F05%2F21%2FELK-Sentinl%2F</url>
    <content type="text"><![CDATA[Sentinl简介Sentinl 5扩展自Kibi / Kibana 5，具有警报和报告功能，可使用标准查询，可编程验证器和各种可配置操作来监控，通知和报告数据系列更改 - 将其视为一个独立的“观察者” “报告”功能（PNG / PDFs快照）。 SENTINEL还旨在通过直接在Kibana UI中整合来简化在Kibi / Kibana中创建和管理警报和报告的过程。 功能模块WatchersAlarmsReportsWatchers是Sentinl核心，主要由 input,Condition,Transform,Actions几大块组成，可以和X-Pack一一对应，部分文档可参考X-Pack，但需要注意的是它和X-Pack还有一些区别，主要体现在input只实现了search，其他并未实现，Actions也并未都实现 安装与配置 安装 /usr/share/kibana/bin/kibana-plugin install https://github.com/sirensolutions/sentinl/releases/download/tag-5.5/sentinl-v5.6.5.zip config kibana.yml config: 12345678910111213141516171819202122232425262728sentinl: es: timefield: '@timestamp' default_index: watcher type: watch alarm_index: watcher_alarms sentinl: history: 20 results: 50 settings: email: active: false user: username password: password host: smtp.server.com ssl: true timeout: 10000 # mail server connection timeout slack: active: false username: username hook: 'https://hooks.slack.com/services/&lt;token&gt;' channel: '#channel' report: active: false tmp_path: /tmp/ pushapps: active: false api_key: '&lt;pushapps API Key&gt;' raw 1234567891011121314151617181920212223242526272829303132333435363738394041"input": &#123; "search": &#123; "request": &#123; "index": [ "&lt;xxx-&#123;now/d&#125;&gt;" ], "body": &#123; "query": &#123; "bool": &#123; "should": [ &#123; "match": &#123; "status": "502" &#125; &#125;, &#123; "match": &#123; "status": "404" &#125; &#125; ], "minimum_should_match": 1, #must setup "filter": &#123; "range": &#123; "@timestamp": &#123; "gte": "now-60s", "lte": "now" &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;, "condition": &#123; "script": &#123; "script": "payload.hits.total &gt; 30" &#125; &#125;,]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES内置账号密码修改、自定义角色自定义账号、ldap及AD认证]]></title>
    <url>%2F2018%2F05%2F07%2Fes%2F</url>
    <content type="text"><![CDATA[自定义内置账号 账户elastic为elasticsearch超级管理员，拥有所有权限 账户kibana用于kibana组件获取相关信息用于web展示 账户logstash_system用于logstash服务获取elasticsearch的监控数据 注意：此步骤需先启动elasticsearch服务 12345678910111213141516[elasticsearch@elasticsearch elasticsearch-6.0.0]$ ./bin/x-pack/setup-passwords interactiveInitiating the setup of reserved user elastic,kibana,logstash_system passwords.You will be prompted to enter passwords as the process progresses.Please confirm that you would like to continue [y/N]yEnter password for [elastic]: Reenter password for [elastic]: Enter password for [kibana]: Reenter password for [kibana]: Enter password for [logstash_system]: Reenter password for [logstash_system]: Changed password for user [kibana]Changed password for user [logstash_system]Changed password for user [elastic][elasticsearch@elasticsearch elasticsearch-6.0.0]$ 验证内置账户访问 若不提供用户名密码则返回401 1234567891011121314151617181920[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl 'http://10.59.30.96:9200/_cat/indices?pretty'&#123; "error" : &#123; "root_cause" : [ &#123; "type" : "security_exception", "reason" : "missing authentication token for REST request [/_cat/indices?pretty]", "header" : &#123; "WWW-Authenticate" : "Basic realm=\"security\" charset=\"UTF-8\"" &#125; &#125; ], "type" : "security_exception", "reason" : "missing authentication token for REST request [/_cat/indices?pretty]", "header" : &#123; "WWW-Authenticate" : "Basic realm=\"security\" charset=\"UTF-8\"" &#125; &#125;, "status" : 401&#125; 提供相应用户信息后可访问，若用户权限不足则返回403 使用logstash_system用户访问 123456789101112131415[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl 'http://10.59.30.96:9200/_cat/indices?pretty' -u logstash_system:logstash_system&#123; "error" : &#123; "root_cause" : [ &#123; "type" : "security_exception", "reason" : "action [indices:monitor/stats] is unauthorized for user [logstash_system]" &#125; ], "type" : "security_exception", "reason" : "action [indices:monitor/stats] is unauthorized for user [logstash_system]" &#125;, "status" : 403&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ 使用kibana用户访问 12345678[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl 'http://10.59.30.96:9200/_cat/indices?pretty' -u kibana:kibanayellow open .monitoring-es-6-2018.01.10 nND6-i_rR5iLEYVccBGj8w 1 1 yellow open .triggered_watches BtygGZisSDqiL3Y2TaQGqQ 1 1 green open .security-6 QVRL1mcFSAilryHGEhen7Q 1 0 yellow open .watcher-history-6-2018.01.10 SBGiHDAnTPiXFoHU65VY_g 1 1 yellow open .watches kMzN4j5cQySZQQSDVPww8w 1 1 yellow open .monitoring-alerts-6 VygY6VN9R3S0PR_jrGy50Q 1 1 [elasticsearch@elasticsearch elasticsearch-6.0.0]$ 添加自定义角色 添加角色接口为 POST /_xpack/security/role/ 下述示例为添加超级管理员角色的方法 1234567891011121314151617181920212223242526272829303132333435363738394041[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XPOST -H 'Content-type: application/json' -u elastic:elastic 'http://10.59.30.96:9200/_xpack/security/role/admin?pretty' -d '&#123;&gt; "run_as": [ "elastic" ],&gt; "cluster": [ "all" ],&gt; "indices": [&gt; &#123;&gt; "names": [ "*" ],&gt; "privileges": [ "all" ]&gt; &#125;&gt; ]&gt; &#125;'&#123; "role" : &#123; "created" : true &#125;&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XGET -H 'Content-type: application/json' -u elastic:elastic 'http://10.59.30.96:9200/_xpack/security/role/admin?pretty'&#123; "admin" : &#123; "cluster" : [ "all" ], "indices" : [ &#123; "names" : [ "*" ], "privileges" : [ "all" ] &#125; ], "run_as" : [ "elastic" ], "metadata" : &#123; &#125;, "transient_metadata" : &#123; "enabled" : true &#125; &#125;&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ 添加自定义账户 添加用户接口为 POST /_xpack/security/user/ 下述为添加martin账户并添加至admin角色操作方法 1234567891011121314151617181920212223242526272829303132[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XPOST -H 'Content-type: application/json' -u elastic:elastic 'http://10.59.30.96:9200/_xpack/security/user/martin?pretty' -d '&#123;&gt; "password" : "123456",&gt; "full_name" : "Martin Lei",&gt; "roles" : ["admin"],&gt; "email" : "martin@martin.com"&gt; &#125;'&#123; "user" : &#123; "created" : true &#125;&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XGET -H 'Content-type: application/json' -u elastic:elastic 'http://10.59.30.96:9200/_xpack/security/user/martin?pretty'&#123; "rocshen" : &#123; "username" : "martin", "roles" : [ "admin" ], "full_name" : "Martin Lei", "email" : "martin@martin.com", "metadata" : &#123; &#125;, "enabled" : true &#125;&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XGET -H 'Content-type: application/json' -u martin:123456 'http://10.59.30.96:9200/_cat/indices?pretty'yellow open .monitoring-es-6-2018.01.10 nND6-i_rR5iLEYVccBGj8w 1 1 4883 88 2.5mb 2.5mbyellow open .triggered_watches BtygGZisSDqiL3Y2TaQGqQ 1 1 0 0 24.2kb 24.2kbgreen open .security-6 QVRL1mcFSAilryHGEhen7Q 1 0 yellow open .watcher-history-6-2018.01.10 SBGiHDAnTPiXFoHU65VY_g 1 1 630 0 703.3kb 703.3kbyellow open .watches kMzN4j5cQySZQQSDVPww8w 1 1 5 0 33.3kb 33.3kbyellow open .monitoring-alerts-6 VygY6VN9R3S0PR_jrGy50Q 1 1 1 0 6.5kb 6.5kb[elasticsearch@elasticsearch elasticsearch-6.0.0]$ 修改账户密码 修改密码需使用超级管理员权限即elastic账户，接口为POST _xpack/security/user//_password curl参数含义如下 -XPOST 使用post方法传递参数 -H 指定http协议的header信息 -u 指定用于认证的用户信息用户名与密码使用冒号分隔 -d 指定具体要传递的参数信息 12[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XPOST -H 'Content-type: application/json' -u elastic:elastic 'http://10.59.30.96:9200/_xpack/security/user/kibana/_password?pretty' -d '&#123;"password": "123456"&#125;'&#123; &#125; 密码修改后使用老密码访问则返回401，使用更新后的密码则正常 12345678910111213141516171819202122232425262728[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl 'http://10.59.30.96:9200/_cat/indices?pretty' -u kibana:kibana&#123; "error" : &#123; "root_cause" : [ &#123; "type" : "security_exception", "reason" : "failed to authenticate user [kibana]", "header" : &#123; "WWW-Authenticate" : "Basic realm=\"security\" charset=\"UTF-8\"" &#125; &#125; ], "type" : "security_exception", "reason" : "failed to authenticate user [kibana]", "header" : &#123; "WWW-Authenticate" : "Basic realm=\"security\" charset=\"UTF-8\"" &#125; &#125;, "status" : 401&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl 'http://10.59.30.96:9200/_cat/indices?pretty' -u kibana:123456yellow open .monitoring-es-6-2018.01.10 nND6-i_rR5iLEYVccBGj8w 1 1 yellow open .triggered_watches BtygGZisSDqiL3Y2TaQGqQ 1 1 green open .security-6 QVRL1mcFSAilryHGEhen7Q 1 0 yellow open .watcher-history-6-2018.01.10 SBGiHDAnTPiXFoHU65VY_g 1 1 yellow open .watches kMzN4j5cQySZQQSDVPww8w 1 1 yellow open .monitoring-alerts-6 VygY6VN9R3S0PR_jrGy50Q 1 1 [elasticsearch@elasticsearch elasticsearch-6.0.0]$ 配置ldap帐号认证ldap服务安装可参考：https://segmentfault.com/a/11... 添加下述ldap相关述配置 bind_dn为ldap的管理DN bind_password为管理dn的密码 user_search.base_dn为linux系统账户信息导入ldap的信息 user_search.attribute为账户在ldap中的标识信息 group_search.base_dn为linux系统组信息导入ldap的信息 12345678910111213141516171819202122232425262728[elasticsearch@elasticsearch elasticsearch-6.0.0]$ vim config/elasticsearch.yml ......network.host: 10.59.30.96bootstrap.system_call_filter: falsexpack.ssl.key: elasticsearch/elasticsearch.keyxpack.ssl.certificate: elasticsearch/elasticsearch.crtxpack.ssl.certificate_authorities: ca/ca.crtxpack.security.transport.ssl.enabled: truexpack: security: authc: realms: ldap1: type: ldap order: 0 url: "ldap://10.59.30.95" bind_dn: "cn=Manager, dc=martin, dc=com" bind_password: 123456 user_search: base_dn: "ou=People,dc=martin,dc=com" attribute: uid group_search: base_dn: "ou=Group,dc=martin,dc=com" unmapped_groups_as_roles: false 配置AD域帐号认证添加下ldap相关述配置至elasticsearch.yml，此处为接着上述LDAP配置添加，如果只需配置AD认证请将ldap相关配置删除即可； domain_name为AD域的域名 url为AD域的地址 bind_dnw为随意的域账户名称（格式为user@domain） bind_password为上述账户的密码 1234567891011121314151617181920212223xpack: security: authc: realms: ldap1: type: ldap order: 0 url: "ldap://10.59.30.94" bind_dn: "cn=Manager, dc=martin, dc=com" bind_password: 123456 user_search: base_dn: "ou=People,dc=martin,dc=com" attribute: uid group_search: base_dn: "ou=Group,dc=martin,dc=com" unmapped_groups_as_roles: false active_directory: type: active_directory order: 1 domain_name: martin.com url: ldap://ad.martin.com bind_dn: martin@martin.com bind_password: AD.123456 重启elasticsearch服务并使用ldap域账户user01登录 1234567891011121314151617181920212223242526272829303132[elasticsearch@elasticsearch elasticsearch-6.0.0]$ killall java[elasticsearch@elasticsearch elasticsearch-6.0.0]$ ./bin/elasticsearch -d[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XGET -u user01:user01 'http://10.59.30.96:9200/_cat?pretty'=^.^=/_cat/allocation/_cat/shards/_cat/shards/&#123;index&#125;/_cat/master/_cat/nodes/_cat/tasks/_cat/indices/_cat/indices/&#123;index&#125;/_cat/segments/_cat/segments/&#123;index&#125;/_cat/count/_cat/count/&#123;index&#125;/_cat/recovery/_cat/recovery/&#123;index&#125;/_cat/health/_cat/pending_tasks/_cat/aliases/_cat/aliases/&#123;alias&#125;/_cat/thread_pool/_cat/thread_pool/&#123;thread_pools&#125;/_cat/plugins/_cat/fielddata/_cat/fielddata/&#123;fields&#125;/_cat/nodeattrs/_cat/repositories/_cat/snapshots/&#123;repository&#125;/_cat/templates[elasticsearch@elasticsearch elasticsearch-6.0.0]$ 使用AD域账户martin登录 123456789101112131415161718192021222324252627282930[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl http://10.59.30.96:9200/_cat?pretty -u martin:AD.123456=^.^=/_cat/allocation/_cat/shards/_cat/shards/&#123;index&#125;/_cat/master/_cat/nodes/_cat/tasks/_cat/indices/_cat/indices/&#123;index&#125;/_cat/segments/_cat/segments/&#123;index&#125;/_cat/count/_cat/count/&#123;index&#125;/_cat/recovery/_cat/recovery/&#123;index&#125;/_cat/health/_cat/pending_tasks/_cat/aliases/_cat/aliases/&#123;alias&#125;/_cat/thread_pool/_cat/thread_pool/&#123;thread_pools&#125;/_cat/plugins/_cat/fielddata/_cat/fielddata/&#123;fields&#125;/_cat/nodeattrs/_cat/repositories/_cat/snapshots/&#123;repository&#125;/_cat/templates[elasticsearch@elasticsearch elasticsearch-6.0.0]$ 为域账户信息映射角色接口为：POST /_xpack/security/role_mapping/ 下述为映射user1*账户为管理员角色的操作步骤 1234567891011121314151617181920212223242526272829303132333435363738[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XPOST -H 'Content-type: application/json' -u elastic:elastic 'http://10.59.30.96:9200/_xpack/security/role_mapping/ldap_user_admin?pretty' -d '&#123;&gt; "roles": [ "admin" ],&gt; "enabled": true,&gt; "rules": &#123;&gt; "any": [&gt; &#123;&gt; "field": &#123;&gt; "username": "/user1*/"&gt; &#125;&gt; &#125;&gt; ]&gt; &#125;&gt; &#125;'&#123; "role_mapping" : &#123; "created" : true &#125;&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XGET -H 'Content-type: application/json' -u elastic:elastic 'http://10.59.30.96:9200/_xpack/security/role_mapping/ldap_user_admin?pretty'&#123; "ldap_user_admin" : &#123; "enabled" : true, "roles" : [ "admin" ], "rules" : &#123; "any" : [ &#123; "field" : &#123; "username" : "/user1*/" &#125; &#125; ] &#125;, "metadata" : &#123; &#125; &#125;&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ 验证域账户权限，使用user01无权访问indices接口，使用user11可以访问； 12345678910111213141516171819202122[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XGET -u user01:user01 'http://10.59.30.96:9200/_cat/indices?pretty'&#123; "error" : &#123; "root_cause" : [ &#123; "type" : "security_exception", "reason" : "action [cluster:monitor/state] is unauthorized for user [user01]" &#125; ], "type" : "security_exception", "reason" : "action [cluster:monitor/state] is unauthorized for user [user01]" &#125;, "status" : 403&#125;[elasticsearch@elasticsearch elasticsearch-6.0.0]$ curl -XGET -u user11:user11 'http://10.59.30.96:9200/_cat/indices?pretty'yellow open .monitoring-es-6-2018.01.10 nND6-i_rR5iLEYVccBGj8w 1 1 6178 44 5.9mb 5.9mbyellow open .triggered_watches BtygGZisSDqiL3Y2TaQGqQ 1 1 0 0 11.7kb 11.7kbgreen open .security-6 QVRL1mcFSAilryHGEhen7Q 1 0 yellow open .watcher-history-6-2018.01.10 SBGiHDAnTPiXFoHU65VY_g 1 1 777 0 1.1mb 1.1mbyellow open .watches kMzN4j5cQySZQQSDVPww8w 1 1 5 0 40.2kb 40.2kbyellow open .monitoring-alerts-6 VygY6VN9R3S0PR_jrGy50Q 1 1 1 0 12.8kb 12.8kb[elasticsearch@elasticsearch elasticsearch-6.0.0]$ 常见报错No subject alternative names matching IP address 1234[2018-01-10T19:19:35,483][WARN ][o.e.x.s.t.n.SecurityNetty4Transport] [fzP4t-4] exception caught on transport layer [[id: 0x5d97fe48, L:/0:0:0:0:0:0:0:1:49121 ! R:/0:0:0:0:0:0:0:1:9300]], closing connection io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: General SSLEngine problem......Caused by: java.security.cert.CertificateException: No subject alternative names matching IP address 0:0:0:0:0:0:0:1 found 解决方案为一种是关闭IPv6地址，另一种是修改ES_HOME/config/elasticsearch.yml中的network.host值为本机eth0的IP 参考文档 官方安装步骤：https://www.elastic.co/guide/... 配置内置账户密码：https://www.elastic.co/guide/... 修改账户密码：https://www.elastic.co/guide/... 用户相关操作：https://www.elastic.co/guide/... 使用LDAP认证： https://www.elastic.co/guide/... 用户角色映射： https://www.elastic.co/guide/...]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch分片及集群说明]]></title>
    <url>%2F2018%2F05%2F07%2FElasticsearch%E5%88%86%E7%89%87%E5%8F%8A%E9%9B%86%E7%BE%A4%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[replica的作用主要包括： a.容灾:primary分片丢失，replica分片就会被顶上去成为新的主分片，同时根据这个新的主分片创建新的replica,集群数据安然无恙； b.提高查询性能：replica和primary分片的数据是相同的，所以对于一个query既可以查主分片也可以查备分片，在合适的范围内多个replica性能会更优(但要考虑资源占用也会提升[cpu/disk/heap])，另外index request只能发生在主分片上，replica不能执行index request; 分片数目调整：对于一个索引，除非重建索引否则不能调整分片的数目(主分片数，number_of_shards),但可以随时调整replica数(number_of_replicas) ES集群状态有三种： Green: 所有主分片和备份分片都准备就绪(分配成功)，即使有一台机器挂了(假设一台机器一个实例)，数据都不会丢失，但会变成YELLOW状态； Yellow: 所有主分片准备就绪，但存在至少一个主分片(假设是A)对应的备份分片没有就绪，此时集群属于告警状态，意味着集群高可用和容灾能力下降，如果刚好A所在的机器挂了，并且你只设置了一个备份(已处于未就绪状态),那么A的数据就会丢失(查询结果不完整)，此时集群进入Red状态； Red: 至少有一个主分片没有就绪(直接原因是找不到对应的备份分片成为新的主分片),此时查询的结果会出现数据丢失(不完整) Elasticsearch与关系数据的类比对应关系如下：Relational DB ⇒ Databases ⇒ Tables ⇒ Rows ⇒ ColumnsElasticsearch ⇒ Indices ⇒ Types ⇒ Documents ⇒ Fields 这里的document的可以理解为一个JSON序列对象。每个document可包含多个field。再来说说Shard，每个Index（对应Database）包含多个Shard，默认是5个，分散在不同的Node上，但不会存在两个相同的Shard存在一个Node上，这样就没有备份的意义了。Shard是一个最小的Lucene索引单元。当来一个document的时候，Elasticsearch通过对docid进行hash来确定其放在哪个shard上面，然后在shard上面进行索引存储。replicas就是备份，Elasticsearch采用的是Push Replication模式，当你往 master主分片上面索引一个文档，该分片会复制该文档(document)到剩下的所有 replica副本分片中，这些分片也会索引这个文档。 当进行查询时，如果提供了查询的DocID，Elasticsearch通过hash就知道Doc存在哪个shard上面，再通过routing table查询就知道再哪个node上面，让后去node上面去取就好了。如果不提供DocID,那么Elasticsearch会在该Index（indics）shards所在的所有node上执行搜索预警，然后返回搜索结果，由coordinating node gather之后返回给用户。 集群信息说明图如下：]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo修改默认端口]]></title>
    <url>%2F2018%2F04%2F20%2Fhexo%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[默认使用4000端口，用hexo s -p 80 ，可以暂时修改启动端口。 但是每次启动都要写”-p 80”才行，过于繁琐。 修改方法：找到node_modules\hexo-server\index.js文件，可以修改默认的port值！]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker cicd持续集成部署]]></title>
    <url>%2F2018%2F04%2F20%2Fdocker-cicd%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[持续集成的概念持续集成，Continuous integration ，简称CI。 首先，解释下集成：所有的项目代码都是托管在SVN或者GIT服务器上（以下简称代码服务器）。每个项目都有若干个单元测试和集成测试。集成测试是单元测试的逻辑扩展：在单元测试的基础上，将所有模块按照设计要求组装成为子系统或系统进行集成测试。实践表明，一些模块虽然能够单独地工作，但并不能保证连接起来也能正常的工作。一些局部反映不出来的问题，在全局上很可能暴露出来（关于单元测试及集成测试的详述，读者可以查阅相关文档）。 简单来说，集成测试就是把所有的单元测试跑一遍，以及其它一些能自动完成的测试。只有通过了集成测试的代码才能上传到代码服务器上，确保上传的代码没有问题。集成一般指集成测试。 持续，显而易见就是长期对代码进行的集成测试。既然是长期进行，那么最好是自动执行，否则人工执行既没保证，而且耗人力。 基于此种目的，我们需要有一台服务器，它将定期从代码服务器中拉取代码，并进行编译，然后自动运行集成测试；并且每次集成测试的结果都会记录在案。 持续集成的特点 它是一个自动化的周期性的集成测试过程，从拉取代码、编译构建、运行测试、结果记录、测试统计等都是自动完成的，无需人工干预； 需要有专门的集成服务器来执行集成构建； 需要有代码托管工具支持； 持续集成的作用 保证团队开发人员提交代码的质量，减轻了软件发布时的压力； 持续集成中的任何一个环节都是自动完成的，无需太多的人工干预，有利于减少重复过程以节省时间、费用和工作量； 首先，Docker可以让你非常容易和方便地以“容器化”的方式去部署应用。 它就像集装箱一样，打包了所有依赖，再在其他服务器上部署很容易，不至于换服务器后发现各种配置文件散落一地，这样就解决了编译时依赖和运行时依赖的问题； 其次，Docker的隔离性使得应用在运行是就像处于沙箱中一样，每个应用都认为自己是在系统中唯一运行的程序，就像刚才例子中，A依赖于Python 2.7，同时A还依赖于B，但B却依赖于Python3， 这样我们可以在系统中部署一个基于python2.7的容器和一个基于python3的容器，这样就可以很方便的在系统中部署多种不同的环境来解决依赖复杂度的问题。这里有些朋友可能会说，虚拟机也可以解决这样的问题！诚然，虚拟化确实可以做到这一点，但是这样需要硬件支持虚拟化及开启BIOS中虚拟化相关的功能，同时还需要在系统中安装2套操作系统，虚拟机的出现是解决了操作系统和物理机的强耦合问题。但是Docker就轻量化很多，只需内核支持，无需硬件和BIOS的强制要求，可以很轻松迅速的在系统上部署多套不同的容器环境，容器的出现解决了应用和操作系统的强耦合问题。 正以为Docker是以应用为中心，镜像中打包了应用及应用所需的环境，一次构建，处处运行。这种特性完美的解决了传统模式下应用迁移后面临的环境不一致问题。 同时，Docker 压根不管内部应用怎么启动，你自己爱咋来咋来，我们用 docker start 或 run 作为统一标准。这样我们应用启动就标准化了， 不需要再根据不同应用而记忆一大串不同的启动命令。 基于Docker的特征，现在常见的利用 Docker 进行持续集成的流程如下： 开发者提交代码 触发镜像构建 构建镜像上传至私有仓库 镜像下载至执行机器 镜像运行 其基本拓扑结构如下所示： 熟悉Docker的都知道，Docker以的启动是非常快的，可以说是秒启。在上述的五步中，1 和 5 的耗时是比较短的，整个持续集成主要耗时集中在中间的3个步骤，也就是 Docker build，Docker push ，Docekr pull 的时间消耗. Docker Registry升级到 v2 后加入了很多安全相关检查，在v2中的镜像的存储格式变成了gzip ，镜像在压缩过程中占用的时间也是比较多的。 Docker pull 镜像的速度对服务的启动速度至关重要，好在 Registry v2 后可以并行 pull 了，速度有了很大的改善。但是依然有一些小的问题影响了启动的速度： 下载镜像和解压镜像是串行的； 串行解压，由于 v2 都是 gzip,要解压，尽管并行下载了还是串行解压，内网的话解压时间比网络传输都要长； 和 Registry 通信， Registry 在 pull的过程中并不提供下载内容只是提供下载URL和鉴权，这一部分加长网络传输而且一些 Metadata还是要去后端存储获取，延时还是有一些的。 整个持续集成平台架构演进到如下图所示：]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstash吞吐率优化]]></title>
    <url>%2F2018%2F04%2F13%2Flogstash%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[问题一最近发现kibana的日志传的很慢，常常查不到日志，由于所有的日志收集都只传输到了一个logstash进行收集和过滤，于是怀疑是否是由于logstash的吞吐量存在瓶颈。一看，还真是到了瓶颈。 优化过程经过查询logstash完整配置文件，有几个参数需要调整12345678# pipeline线程数，官方建议是等于CPU内核数pipeline.workers: 24# 实际output时的线程数pipeline.output.workers: 24# 每次发送的事件数pipeline.batch.size: 3000# 发送延时pipeline.batch.delay: 5 PS:由于我们的ES集群数据量较大（&gt;28T），所以具体配置数值视自身生产环境 优化结果ES的吞吐由每秒9817/s提升到41183/s,具体可以通过x-pack的monitor查看。 问题二在查看logstash日志过程中，我们看到了大量的以下报错12[2017-03-18T09:46:21,043][INFO ][logstash.outputs.elasticsearch] retrying failed action with response code: 429 (&#123;"type"=&gt;"es_rejected_execution_exception", "reason"=&gt;"rejected execution of org.elasticsearch.transport.TransportService$6@6918cf2e on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@55337655[Running, pool size = 24, active threads = 24, queued tasks = 50, completed tasks = 1767887463]]"&#125;)[2017-03-18T09:46:21,043][ERROR][logstash.outputs.elasticsearch] Retrying individual actions 查询官网，确认为时ES的写入遇到了瓶颈1Make sure to watch for TOO_MANY_REQUESTS (429) response codes (EsRejectedExecutionException with the Java client), which is the way that Elasticsearch tells you that it cannot keep up with the current indexing rate. When it happens, you should pause indexing a bit before trying again, ideally with randomized exponential backoff. 我们首先想到的是来调整ES的线程数，但是官网写到”Don’t Touch There Settings!”, 那怎么办？于是乎官方建议我们修改logstash的参数pipeline.batch.size 在ES5.0以后，es将bulk、flush、get、index、search等线程池完全分离，自身的写入不会影响其他功能的性能。来查询一下ES当前的线程情况：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647GET _nodes/stats/thread_pool?pretty&#123; "_nodes": &#123; "total": 6, "successful": 6, "failed": 0 &#125;, "cluster_name": "dev-elasticstack5.0", "nodes": &#123; "nnfCv8FrSh-p223gsbJVMA": &#123; "timestamp": 1489804973926, "name": "node-3", "transport_address": "192.168.3.***:9301", "host": "192.168.3.***", "ip": "192.168.3.***:9301", "roles": [ "master", "data", "ingest" ], "attributes": &#123; "rack": "r1" &#125;, "thread_pool": &#123; "bulk": &#123; "threads": 24, "queue": 214, "active": 24, "rejected": 30804543, "largest": 24, "completed": 1047606679 &#125;, ...... "watcher": &#123; "threads": 0, "queue": 0, "active": 0, "rejected": 0, "largest": 0, "completed": 0&#125;&#125;&#125;&#125;&#125; 其中：”bulk”模板的线程数24，当前活跃的线程数24，证明所有的线程是busy的状态，queue队列214，rejected为30804543。那么问题就找到了，所有的线程都在忙，队列堵满后再有进程写入就会被拒绝，而当前拒绝数为30804543。 优化方案问题找到了，如何优化呢。官方的建议是提高每次批处理的数量，调节传输间歇时间。当batch.size增大，es处理的事件数就会变少，写入也就越快了。123456vim /etc/logstash/logstash.yml#pipeline.workers: 24pipeline.output.workers: 24pipeline.batch.size: 10000pipeline.batch.delay: 10 具体的worker/output.workers数量建议等于CPU数，batch.size/batch.delay根据实际的数据量逐渐增大来测试最优值。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka性能调优]]></title>
    <url>%2F2018%2F04%2F13%2Fkafka%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Kafka的配置详尽、复杂，想要进行全面的性能调优需要掌握大量信息，这里只记录一下我在日常工作使用中走过的坑和经验来对kafka集群进行优化常用的几点。1.JVM的优化java相关系统自然离不开JVM的优化。首先想到的肯定是Heap Size的调整。 vim bin/kafka-server-start.sh 调整KAFKA_HEAP_OPTS=”-Xmx16G -Xms16G”的值推荐配置：一般HEAP SIZE的大小不超过主机内存的50%。 2.网络和ios操作线程配置优化：1234# broker处理消息的最大线程数num.network.threads=9# broker处理磁盘IO的线程数num.io.threads=16 推荐配置：num.network.threads主要处理网络io，读写缓冲区数据，基本没有io等待，配置线程数量为cpu核数加1。 num.io.threads主要进行磁盘io操作，高峰期可能有些io等待，因此配置需要大些。配置线程数量为cpu核数2倍，最大不超过3倍。 3.socket server可接受数据大小(防止OOM异常)：socket.request.max.bytes=2147483600 推荐配置： 根据自己业务数据包的大小适当调大。这里取值是int类型的，而受限于java int类型的取值范围又不能太大： java int的取值范围为（-2147483648~2147483647），占用4个字节（-2的31次方到2的31次方-1，不能超出，超出之后报错：org.apache.kafka.common.config.ConfigException: Invalid value 8589934592 for configuration socket.request.max.bytes: Not a number of type INT。 4.log数据文件刷盘策略—每当producer写入10000条消息时，刷数据到磁盘—log.flush.interval.messages=10000 —每间隔1秒钟时间，刷数据到磁盘—log.flush.interval.ms=1000 推荐配置： 为了大幅度提高producer写入吞吐量，需要定期批量写文件。一般无需改动，如果topic的数据量较小可以考虑减少log.flush.interval.ms和log.flush.interval.messages来强制刷写数据，减少可能由于缓存数据未写盘带来的不一致。推荐配置分别message 10000，间隔1s。 5.日志保留策略配置—日志保留时长—log.retention.hours=72 —段文件配置—log.segment.bytes=1073741824 推荐配置： 日志建议保留三天，也可以更短；段文件配置1GB，有利于快速回收磁盘空间，重启kafka加载也会加快（kafka启动时是单线程扫描目录(log.dir)下所有数据文件）。如果文件过小，则文件数量比较多。 6.replica复制配置123num.replica.fetchers=3replica.fetch.min.bytes=1replica.fetch.max.bytes=5242880 推荐配置： 每个follow从leader拉取消息进行同步数据，follow同步性能由这几个参数决定，分别为: 拉取线程数(num.replica.fetchers):fetcher配置多可以提高follower的I/O并发度，单位时间内leader持有更多请求，相应负载会增大，需要根据机器硬件资源做权衡，建议适当调大； 最小字节数(replica.fetch.min.bytes):一般无需更改，默认值即可； 最大字节数(replica.fetch.max.bytes)：默认为1MB，这个值太小，推荐5M，根据业务情况调整 最大等待时间(replica.fetch.wait.max.ms):follow拉取频率，频率过高，leader会积压大量无效请求情况，无法进行数据同步，导致cpu飙升。配置时谨慎使用，建议默认值，无需配置。 7.分区数量配置num.partitions=5 推荐配置： 默认partition数量1，如果topic在创建时没有指定partition数量，默认使用此值。Partition的数量选取也会直接影响到Kafka集群的吞吐性能，配置过小会影响消费性能，建议改为5。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 索引查询使用指南]]></title>
    <url>%2F2018%2F04%2F03%2FELK%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[1.我们通常用用_cat API检测集群是否健康。 确保9200端口号可用:curl ‘localhost:9200/_cat/health?v’ 绿色表示一切正常, 黄色表示所有的数据可用但是部分副本还没有分配,红色表示部分数据因为某些原因不可用. 2.通过如下语句，我们可以获取集群的节点列表：curl ‘localhost:9200/_cat/nodes?v’ 3.通过如下语句，列出所有索引：curl ‘localhost:9200/_cat/indices?v’返回结果： 4.创建索引现在我们创建一个名为“customer”的索引，然后再查看所有的索引： curl -XPUT ‘localhost:9200/customer?pretty’ curl ‘localhost:9200/_cat/indices?v’ 结果如下： 上图中红框所表示的是：我们有一个叫customer的索引，它有五个私有的分片以及一个副本，在它里面有0个文档。 5.插入和获取现在我么插入一些数据到集群索引。我们必须给ES指定所以的类型。如下语句：”external” type, ID：1:主体为JSON格式的语句： { “name”: “John Doe” }1234curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '&#123; "name": "John Doe"&#125;' 返回结果为：create：true 表示插入成功。 获取GET，语句如下：1curl -XGET 'localhost:9200/customer/external/1?pretty' 其中含义为：获取customer索引下类型为external，id为1的数据，pretty参数表示返回结果格式美观。 6.删除索引 DELETE12curl -XDELETE 'localhost:9200/customer?pretty'curl 'localhost:9200/_cat/indices?v' 表示索引删除成功。 7.通过以上命令语句的学习，我们发现索引的增删改查有一个类似的格式，总结如下：1234567curl -X&lt;REST Verb&gt; &lt;Node&gt;:&lt;Port&gt;/&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt;&lt;REST Verb&gt;：REST风格的语法谓词&lt;Node&gt;:节点ip&lt;port&gt;:节点端口号，默认9200&lt;Index&gt;:索引名&lt;Type&gt;:索引类型&lt;ID&gt;:操作对象的ID号 8 修改数据12345678curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '&#123; "name": "John Doe"&#125;'curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '&#123; "name": "Jane Doe"&#125;' 上述命令语句是：先新增id为1，name为John Doe的数据，然后将id为1的name修改为Jane Doe。 9.更新数据9.1 这个例子展示如何将id为1文档的name字段更新为Jane Doe：1234curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '&#123; "doc": &#123; "name": "Jane Doe" &#125;&#125;' 9.2 这个例子展示如何将id为1数据的name字段更新为Jane Doe同时增加字段age为20:1234curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '&#123; "doc": &#123; "name": "Jane Doe", "age": 20 &#125;&#125;' 9.3 也可以通过一些简单的scripts来执行更新。一下语句通过使用script将年龄增加5:1234curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '&#123; "script" : "ctx._source.age += 5"&#125;' 10 删除数据删除数据那是相当的直接. 下面的语句将执行删除Customer中ID为2的数据：1curl -XDELETE 'localhost:9200/customer/external/2?pretty' 11 批处理举例:下面语句将在一个批量操作中执行创建索引：123456curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '&#123;"index":&#123;"_id":"1"&#125;&#125;&#123;"name": "John Doe" &#125;&#123;"index":&#123;"_id":"2"&#125;&#125;&#123;"name": "Jane Doe" &#125;' 下面语句批处理执行更新id为1的数据然后执行删除id为2的数据12345curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '&#123;"update":&#123;"_id":"1"&#125;&#125;&#123;"doc": &#123; "name": "John Doe becomes Jane Doe" &#125; &#125;&#123;"delete":&#123;"_id":"2"&#125;&#125;' 12.导入数据集你可以点击这里下载示例数据集:accounts.json其中每个数据都是如下格式:12345678910111213141516&#123; "index":&#123;"_id":"1"&#125;&#125;&#123; "account_number": 0, "balance": 16623, "firstname": "Bradshaw", "lastname": "Mckenzie", "age": 29, "gender": "F", "address": "244 Columbus Place", "employer": "Euron", "email": "bradshawmckenzie@euron.com", "city": "Hobucken", "state": "CO"&#125; 导入示例数据集:12curl -XPOST 'localhost:9200/bank/account/_bulk?pretty' --data-binary "@accounts.json"curl 'localhost:9200/_cat/indices?v' 上图红框表示我们已经成功批量导入1000条数据索引到bank索引中。 13.查询Sample:12345678910111213141516171819202122232425curl 'localhost:9200/bank/_search?q=*&amp;pretty'&#123; "took" : 63, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "failed" : 0 &#125;, "hits" : &#123; "total" : 1000, "max_score" : 1.0, "hits" : [ &#123; "_index" : "bank", "_type" : "account", "_id" : "1", "_score" : 1.0, "_source" : &#123;"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"&#125; &#125;, &#123; "_index" : "bank", "_type" : "account", "_id" : "6", "_score" : 1.0, "_source" : &#123;"account_number":6,"balance":5686,"firstname":"Hattie","lastname":"Bond","age":36,"gender":"M","address":"671 Bristol Street","employer":"Netagy","email":"hattiebond@netagy.com","city":"Dante","state":"TN"&#125; &#125;, &#123; "_index" : "bank", "_type" : "account", 上面示例返回所有bank中的索引数据。其中 q=* 表示匹配索引中所有的数据。 等价于:1234curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125;' 14 查询语言 匹配所有数据，但只返回1个:12345curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "size": 1&#125;' 注意：如果siez不指定，则默认返回10条数据。123456curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "from": 10, "size": 10&#125;' 返回从11到20的数据。（索引下标从0开始）12345curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": &#123; "balance": &#123; "order": "desc" &#125; &#125;&#125;' 上述示例匹配所有的索引中的数据，按照balance字段降序排序，并且返回前10条（如果不指定size，默认最多返回10条）。 15.执行搜索 下面例子展示如何返回两个字段（account_number balance）12345curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "_source": ["account_number", "balance"]&#125;' 返回account_number 为20 的数据:1234curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match": &#123; "account_number": 20 &#125; &#125;&#125;' 返回address中包含mill的所有数据：1234curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match": &#123; "address": "mill" &#125; &#125;&#125;' 返回地址中包含mill或者lane的所有数据：1234curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match": &#123; "address": "mill lane" &#125; &#125;&#125;' 和上面匹配单个词语不同，下面这个例子是多匹配（match_phrase短语匹配），返回地址中包含短语 “mill lane”的所有数据：1234curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "match_phrase": &#123; "address": "mill lane" &#125; &#125;&#125;' 以下是布尔查询，布尔查询允许我们将多个简单的查询组合成一个更复杂的布尔逻辑查询。这个例子将两个查询组合，返回地址中含有mill和lane的所有记录数据：1234567891011curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' 上述例子中，must表示所有查询必须都为真才被认为匹配。 相反, 这个例子组合两个查询，返回地址中含有mill或者lane的所有记录数据：1234567891011curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "bool": &#123; "should": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' 上述例子中，bool表示查询列表中只要有任何一个为真则认为匹配。 下面例子组合两个查询，返回地址中既没有mill也没有lane的所有数据：1234567891011curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "bool": &#123; "must_not": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' 上述例子中,must_not表示查询列表中没有为真的（也就是全为假）时则认为匹配。 我们可以组合must、should、must_not来实现更加复杂的多级逻辑查询。 下面这个例子返回年龄大于40岁、不居住在ID的所有数据：12345678910111213curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "age": "40" &#125; &#125; ], "must_not": [ &#123; "match": &#123; "state": "ID" &#125; &#125; ] &#125; &#125;&#125;' 16.过滤filter(查询条件设置) 下面这个例子使用了布尔查询返回balance在20000到30000之间的所有数据。12345678910111213141516curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "query": &#123; "bool": &#123; "must": &#123; "match_all": &#123;&#125; &#125;, "filter": &#123; "range": &#123; "balance": &#123; "gte": 20000, "lte": 30000 &#125; &#125; &#125; &#125; &#125;&#125;' 17 聚合 Aggregations下面这个例子： 将所有的数据按照state分组（group），然后按照分组记录数从大到小排序，返回前十条（默认）：1234567891011curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state" &#125; &#125; &#125;&#125;' 注意：我们设置size=0，不显示查询hits，因为我们只想看返回的聚合结果。 上述语句类似于以下SQL语句：SELECT state, COUNT() FROM bank GROUP BY state ORDER BY COUNT() DESC 下面这个实例按照state分组，降序排序，返回balance的平均值：123456789101112131415161718curl -XPOST 'localhost:9200/bank/_search?pretty' -d '&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state" &#125;, "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125;&#125;']]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK的一次吞吐量优化]]></title>
    <url>%2F2018%2F04%2F03%2Felk%2F</url>
    <content type="text"><![CDATA[问题一 ● 最近发现kibana的日志传的很慢，常常查不到日志，由于所有的日志收集都只传输到了一个logstash进行收集和过滤，于是怀疑是否是由于logstash的吞吐量存在瓶颈。一看，还真是到了瓶颈。 ● 优化过程 ● 经过查询logstash完整配置文件，有几个参数需要调整 12345678# pipeline线程数，官方建议是等于CPU内核数pipeline.workers: 24# 实际output时的线程数pipeline.output.workers: 24# 每次发送的事件数pipeline.batch.size: 3000# 发送延时pipeline.batch.delay: 5 PS:由于我们的ES集群数据量较大（&gt;28T），所以具体配置数值视自身生产环境 问题二 ● 在查看logstash日志过程中，我们看到了大量的以下报错12[2017-03-18T09:46:21,043][INFO ][logstash.outputs.elasticsearch] retrying failed action with response code: 429 (&#123;"type"=&gt;"es_rejected_execution_exception", "reason"=&gt;"rejected execution of org.elasticsearch.transport.TransportService$6@6918cf2e on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@55337655[Running, pool size = 24, active threads = 24, queued tasks = 50, completed tasks = 1767887463]]"&#125;)[2017-03-18T09:46:21,043][ERROR][logstash.outputs.elasticsearch] Retrying individual actions ● 查询官网，确认为时ES的写入遇到了瓶颈 ● Make sure to watch for TOO_MANY_REQUESTS (429) response codes (EsRejectedExecutionException with the Java client), which is the way that Elasticsearch tells you that it cannot keep up with the current indexing rate. When it happens, you should pause indexing a bit before trying again, ideally with randomized exponential backoff. 我们首先想到的是来调整ES的线程数，但是官网写到”Don’t Touch There Settings!”, 那怎么办？于是乎官方建议我们修改logstash的参数pipeline.batch.size ● 在ES5.0以后，es将bulk、flush、get、index、search等线程池完全分离，自身的写入不会影响其他功能的性能。来查询一下ES当前的线程情况：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849GET _nodes/stats/thread_pool?pretty可以看到：&#123; "_nodes": &#123; "total": 6, "successful": 6, "failed": 0 &#125;, "cluster_name": "dev-elasticstack5.0", "nodes": &#123; "nnfCv8FrSh-p223gsbJVMA": &#123; "timestamp": 1489804973926, "name": "node-3", "transport_address": "192.168.3.***:9301", "host": "192.168.3.***", "ip": "192.168.3.***:9301", "roles": [ "master", "data", "ingest" ], "attributes": &#123; "rack": "r1" &#125;, "thread_pool": &#123; "bulk": &#123; "threads": 24, "queue": 214, "active": 24, "rejected": 30804543, "largest": 24, "completed": 1047606679 &#125;, ...... "watcher": &#123; "threads": 0, "queue": 0, "active": 0, "rejected": 0, "largest": 0, "completed": 0&#125;&#125;&#125;&#125;&#125; 其中：”bulk”模板的线程数24，当前活跃的线程数24，证明所有的线程是busy的状态，queue队列214，rejected为30804543。那么问题就找到了，所有的线程都在忙，队列堵满后再有进程写入就会被拒绝，而当前拒绝数为30804543。优化方案问题找到了，如何优化呢。官方的建议是提高每次批处理的数量，调节传输间歇时间。当batch.size增大，es处理的事件数就会变少，写入也就愉快了。 123456vim /etc/logstash/logstash.yml#pipeline.workers: 24pipeline.output.workers: 24pipeline.batch.size: 10000pipeline.batch.delay: 10 具体的worker/output.workers数量建议等于CPU数，batch.size/batch.delay根据实际的数据量逐渐增大来测试最优值。]]></content>
  </entry>
  <entry>
    <title><![CDATA[23种非常有用的ElasticSearch查询例子]]></title>
    <url>%2F2018%2F04%2F02%2Fasd%2F</url>
    <content type="text"><![CDATA[一、新建索引为了展示Elasticsearch中不同查询的用法，先在Elasticsearch里面创建了book相关的documents，每本书主要涉及以下字段： title, authors, summary, publish_date(发行日期),publisher以及评论条数。 操作如下：123456789101112curl -XPUT 'https://www.iteblog.com:9200/iteblog_book_index' -d '&#123; "settings": &#123; "number_of_shards": 1 &#125;&#125;'curl -XPOST 'https://www.iteblog.com:9200/iteblog_book_index/book/_bulk' -d '&#123; "index": &#123; "_id": 1 &#125;&#125;&#123; "title": "Elasticsearch: The Definitive Guide", "authors": ["clinton gormley", "zachary tong"], "summary" : "A distibuted real-time search and analytics engine", "publish_date" : "2015-02-07", "num_reviews": 20, "publisher": "oreilly" &#125;&#123; "index": &#123; "_id": 2 &#125;&#125;&#123; "title": "Taming Text: How to Find, Organize, and Manipulate It", "authors": ["grant ingersoll", "thomas morton", "drew farris"], "summary" : "organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization", "publish_date" : "2013-01-24", "num_reviews": 12, "publisher": "manning" &#125;&#123; "index": &#123; "_id": 3 &#125;&#125;&#123; "title": "Elasticsearch in Action", "authors": ["radu gheorge", "matthew lee hinman", "roy russo"], "summary" : "build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms", "publish_date" : "2015-12-03", "num_reviews": 18, "publisher": "manning" &#125;&#123; "index": &#123; "_id": 4 &#125;&#125;&#123; "title": "Solr in Action", "authors": ["trey grainger", "timothy potter"], "summary" : "Comprehensive guide to implementing a scalable search engine using Apache Solr", "publish_date" : "2014-04-05", "num_reviews": 23, "publisher": "manning" &#125;' 通过dev tools来模拟则为：http://cos.leiyawu.com/img/elk_index_check_1.png 123456789POST /iteblog_book_index/book/_bulk&#123; "index": &#123; "_id": 1 &#125;&#125;&#123; "title": "Elasticsearch: The Definitive Guide", "authors": ["clinton gormley", "zachary tong"], "summary" : "A distibuted real-time search and analytics engine", "publish_date" : "2015-02-07", "num_reviews": 20, "publisher": "oreilly" &#125;&#123; "index": &#123; "_id": 2 &#125;&#125;&#123; "title": "Taming Text: How to Find, Organize, and Manipulate It", "authors": ["grant ingersoll", "thomas morton", "drew farris"], "summary" : "organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization", "publish_date" : "2013-01-24", "num_reviews": 12, "publisher": "manning" &#125;&#123; "index": &#123; "_id": 3 &#125;&#125;&#123; "title": "Elasticsearch in Action", "authors": ["radu gheorge", "matthew lee hinman", "roy russo"], "summary" : "build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms", "publish_date" : "2015-12-03", "num_reviews": 18, "publisher": "manning" &#125;&#123; "index": &#123; "_id": 4 &#125;&#125;&#123; "title": "Solr in Action", "authors": ["trey grainger", "timothy potter"], "summary" : "Comprehensive guide to implementing a scalable search engine using Apache Solr", "publish_date" : "2014-04-05", "num_reviews": 23, "publisher": "manning" &#125; ES中的查询请求有两种方式，一种是简易版的查询，另外一种是使用JSON完整的请求体，叫做结构化查询（DSL）。由于DSL查询更为直观也更为简易，所以大都使用这种方式。DSL查询是POST过去一个json，由于post的请求是json格式的，所以存在很多灵活性，也有很多形式。 基本匹配查询主要形式：（1）、使用Search Lite API，并将所有的搜索参数都通过URL传递（2）、使用Elasticsearch DSL，其可以通过传递一个JSON请求来获取结果。Curl方式与其类似，只是提交方式不是POST，而是XGET，提交参数与DSL提交一致 二、基本匹配查询(Basic Match Query)1、在所有的字段中搜索带有”guide”的结果：通过dev tools:GET /iteblog_book_index/book/_search?q=guide 通过curl方式：curl -u elastic “http://10.104.37.115:9281/iteblog_book_index/book/_search?pretty&quot; -d ‘{ “query”: { “multi_match” : { “query” : “guide”, “fields” : [“_all”] } }}’ 通过DSL：(POST json方式) 其输出和上面使用/iteblog_book_index/book/_search?q=guide的输出一样。上面的multi_match关键字通常在查询多个fields的时候作为match关键字的简写方式。fields属性指定需要查询的字段，如果我们想查询所有的字段，这时候可以使用_all关键字，正如上面的一样。 如果只是查询summary字段，则为： title的Guide则不会显示。 2、以上两种方式都允许我们指定查询哪些字段。比如，我们想查询title中出现in Action的图书，那么我们可以这么查询： GET /iteblog_book_index/book/_search?q=title:in%20action 然而，DSL方式提供了更加灵活的方式来构建更加复杂的查询（我们将在后面看到），甚至指定你想要的返回结果。下面的例子中，我将指定需要返回结果的数量，开始的偏移量（这在分页的情况下非常有用），需要返回document中的哪些字段以及高亮关键字： curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “match” : { “title” : “in action” } }, “size”: 2, #返回结果的数量 “from”: 0, #开始的偏移量 “_source”: [ “title”, “summary”, “publish_date” ], “highlight”: { “fields” : { “title” : {} } }}’ 三、Multi-field Search正如我们之前所看到的，想在一个搜索中查询多个 document field （比如使用同一个查询关键字同时在title和summary中查询），你可以使用multi_match查询，使用如下：curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “multi_match” : { “query” : “elasticsearch guide”, “fields”: [“title”, “summary”] } }}’ 四、Boosting上面使用同一个搜索请求在多个field中查询，你也许想提高某个field的查询权重。在下面的例子中，我们把summary field的权重调成3，这样就提高了其在结果中的权重，这样把_id=4的文档相关性大大提高了，如下： curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “multi_match” : { “query” : “elasticsearch guide”, “fields”: [“title”, “summary^3”] } }, “_source”: [“title”, “summary”, “publish_date”]}’ 需要注意的是：Boosting不仅仅意味着计算出来的分数(calculated score)直接乘以boost factor，最终的boost value会经过归一化以及其他一些内部的优化 五、Bool Query在查询条件中使用AND/OR/NOT操作符，这就是布尔查询(Bool Query)。布尔查询可以接受一个must参数(等价于AND)，一个must_not参数(等价于NOT)，以及一个should参数(等价于OR)。比如，我想查询title中出现Elasticsearch或者Solr关键字的图书，图书的作者是clinton gormley，但没有radu gheorge，可以这么来查询： curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “bool”: { “must”: { “bool” : { “should”: [ { “match”: { “title”: “Elasticsearch” }}, { “match”: { “title”: “Solr” }} ] } }, “must”: { “match”: { “authors”: “clinton gormely” }}, “must_not”: { “match”: {“authors”: “radu gheorge” }} } }}’ 六、Fuzzy Queries（模糊查询）模糊查询可以在Match和 Multi-Match查询中使用以便解决拼写的错误，模糊度是基于Levenshtein distance计算与原单词的距离。使用如下：curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “multi_match” : { “query” : “comprihensiv guide”, “fields”: [“title”, “summary”], “fuzziness”: “AUTO” } }, “_source”: [“title”, “summary”, “publish_date”], “size”: 1}’ 需要注意：上面我们将fuzziness的值指定为AUTO，其在term的长度大于5的时候相当于指定值为2。然而80%的人拼写错误的编辑距离(edit distance)为1，所有如果你将fuzziness设置为1可能会提高你的搜索性能。 七、Wildcard Query(通配符查询)通配符查询允许我们指定一个模式来匹配，而不需要指定完整的term。?将会匹配一个字符；将会匹配零个或者多个字符。比如我们想查找所有作者名字中以t字符开始的记录，我们可以如下使用：curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “wildcard” : { #wildcard是通配符意思 “authors” : “t“ } }, “_source”: [“title”, “authors”], “highlight”: { “fields” : { “authors” : {} } }}’ 八、Regexp Query(正则表达式查询)ElasticSearch还支持正则表达式查询，此方式提供了比通配符查询更加复杂的模式。比如我们先查找作者名字以t字符开头，中间是若干个a-z之间的字符，并且以字符y结束的记录，可以如下查询： curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “regexp” : { “authors” : “t[a-z]*y” } }, “_source”: [“title”, “authors”], “highlight”: { “fields” : { “authors” : {} } }}’ 九、Match Phrase Query(匹配短语查询)匹配短语查询要求查询字符串中的trems要么都出现Document中、要么trems按照输入顺序依次出现在结果中。在默认情况下，查询输入的trems必须在搜索字符串紧挨着出现，否则将查询不到。不过我们可以指定slop参数，来控制输入的trems之间有多少个单词仍然能够搜索到，如下所示：curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “multi_match”: { “query”: “search engine”, “fields”: [ “title”, “summary” ], “type”: “phrase”, “slop”: 3 } }, “_source”: [ “title”, “summary”, “publish_date” ]}’ 从上面的例子可以看出，id为4的document被搜索（summary字段里面精确匹配到了search engine），并且分数比较高；而id为1的document也被搜索到了，虽然其summary中的search和engine单词并不是紧挨着的，但是我们指定了slop属性，所以被搜索到了。如果我们将”slop”: 3条件删除，那么id为1的文档将不会被搜索到，如下： 十、Simple Query String(简单查询字符串)simple_query_string是query_string的另一种版本，其更适合为用户提供一个搜索框中，因为其使用+/|/- 分别替换AND/OR/NOT，如果用输入了错误的查询，其直接忽略这种情况而不是抛出异常。使用如下：(注意是POST)curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search{ “query”: { “simple_query_string” : { “query”: “(saerch~1 algorithm~1) + (grant ingersoll) | (tom morton)”, “fields”: [“_all”, “summary^2”] } }, “_source”: [ “title”, “summary”, “authors” ], “highlight”: { “fields” : { “summary” : {} } }} 十一、Term/Terms Query前面的例子中我们已经介绍了全文搜索(full-text search)，但有时候我们对结构化搜索中能够精确匹配并返回搜索结果更感兴趣。这种情况下我们可以使用term和terms查询。在下面例子中，我们想搜索所有曼宁出版社(Manning Publications)出版的图书： curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search -d ‘{ “query”: { “term” : { “publisher”: “manning” } }, “_source” : [“title”,”publish_date”,”publisher”]}’ 还可以使用terms关键字来指定多个terms，如下： { “query”: { “terms” : { “publisher”: [“oreilly”, “packt”] } }} 十二、Term Query - Sorted词查询结果和其他查询结果一样可以很容易地对其进行排序，而且我们可以对输出结果按照多层进行排序：curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search{ “query”: { “term” : { “publisher”: “manning” } }, “_source” : [“title”,”publish_date”,”publisher”], “sort”: [ { “publish_date”: {“order”:”desc”}}, { “title”: { “order”: “desc” }} ]} 执行提示：Fielddata is disabled on text fields by default. Set fielddata=true on [title] in order to load fielddata in memory by uninverting the inverted index 应该是5.x后对排序，聚合这些操作用单独的数据结构(fielddata)缓存到内存里了，需要单独开启 PUT /iteblog_book_index/_mapping/book{ “properties”: { “title”: { “type”: “text”, “fielddata”: true } }} 再次执行： 十三、Range Query(范围查询)另一种结构化查询就是范围查询。在下面例子中，我们搜索所有发行年份为2015的图书：curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search{ “query”: { “range” : { “publish_date”: { “gte”: “2015-01-01”, “lte”: “2015-12-31” } } }, “_source” : [“title”,”publish_date”,”publisher”]} 十四、Filtered Query(过滤查询)过滤查询允许我们对查询结果进行筛选。比如：我们查询标题和摘要中包含Elasticsearch关键字的图书，但是我们想过滤出评论大于20的结果，可以如下使用： curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search{ “query”: { “filtered”: { “query” : { “multi_match”: { “query”: “elasticsearch”, “fields”: [“title”,”summary”] } }, “filter”: { “range” : { “num_reviews”: { “gte”: 20 } } } } }, “_source” : [“title”,”summary”,”publisher”, “num_reviews”]}]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo使用进阶]]></title>
    <url>%2F2018%2F03%2F19%2Fhexo%E4%BD%BF%E7%94%A8%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[hexo 一、后端管理插件hexo-admin插件可以直接在网页端创建、编辑markdown文章内容，并将内容发布到_posts里。另外，对我而言，最方便的是可以很方便的给文章加标题、分类、打标签。 参见： An Admin Interface for Hexohexo-admin in github 1.安装 (1)npm install –save hexo-admin (2)hexo server -d (3)open http://localhost:4000/admin/ 2.配置 在_config.yml最后添加类似如下内容： admin: username: myfavoritename password_hash: be121740bf988b2225a313fa1f107ca1 secret: a secret something username：后端登录用户名 password_hash：后端登录用户密码对应的md5 hash值 secret：用于保证cookie安全 3.预览]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo fs.SyncWriteStream is deprecated]]></title>
    <url>%2F2018%2F02%2F28%2Fhexo-fs-SyncWriteStream-is-deprecated%2F</url>
    <content type="text"><![CDATA[fs.SyncWriteStream is deprecated出现这个错误需要更新hexo-fs插件 使用npm install hexo-fs –save 在执行hexo命令的时候，总会显示如下报错：(node:7048) [DEP0061] DeprecationWarning: fs.SyncWriteStream is deprecated. 从报错信息来看是因为fs.SyncWriteStream is deprecated，node.js从8.0开始已经弃用了fs.SyncWriteStream方法，所以是因为我们node_modules中某个插件调用了这个方法，通过查看Hexo作者GitHub对应的项目，在issue中看到有人提到这个问题，在hexo项目中其中有一个hexo-fs的插件调用了这个方法，所以需要更新hexo-fs插件，更新方法如下： npm install hexo-fs –save 更新插件后问题依然无法解决。 通过–debug来查看：[root@server init]# hexo –debug06:55:32.711 DEBUG Hexo version: 3.5.006:55:32.714 DEBUG Working directory: /data/wwwroot/init/06:55:32.787 DEBUG Config loaded: /data/wwwroot/init/_config.yml06:55:32.832 DEBUG Plugin loaded: hexo-admin(node:25414) [DEP0061] DeprecationWarning: fs.SyncWriteStream is deprecated. 问题出在：hexo-admin的hexo-fs因hexo-admin作为后台管理，无法npm uninstall hexo-admin卸载,则找到对应文件，注释： [root@server init]# grep -irn “SyncWriteStream” ./node_modules/hexo-admin/./node_modules/hexo-admin/node_modules/hexo-fs/lib/fs.js:718:exports.SyncWriteStream = fs.SyncWriteStream;[root@lywserver init]# 将对应的exports.SyncWriteStream = fs.SyncWriteStream;注释(前面 //)即可！]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>SyncWriteStream </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK x-pack 详细说明]]></title>
    <url>%2F2018%2F02%2F26%2Fname%2F</url>
    <content type="text"><![CDATA[我还能说什么呢？？]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>x-pack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
