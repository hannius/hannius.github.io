<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[logstash吞吐率优化]]></title>
    <url>%2F2018%2F04%2F13%2Flogstash%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[问题一最近发现kibana的日志传的很慢，常常查不到日志，由于所有的日志收集都只传输到了一个logstash进行收集和过滤，于是怀疑是否是由于logstash的吞吐量存在瓶颈。一看，还真是到了瓶颈。 优化过程经过查询logstash完整配置文件，有几个参数需要调整12345678# pipeline线程数，官方建议是等于CPU内核数pipeline.workers: 24# 实际output时的线程数pipeline.output.workers: 24# 每次发送的事件数pipeline.batch.size: 3000# 发送延时pipeline.batch.delay: 5 PS:由于我们的ES集群数据量较大（&gt;28T），所以具体配置数值视自身生产环境 优化结果ES的吞吐由每秒9817/s提升到41183/s,具体可以通过x-pack的monitor查看。 问题二在查看logstash日志过程中，我们看到了大量的以下报错12[2017-03-18T09:46:21,043][INFO ][logstash.outputs.elasticsearch] retrying failed action with response code: 429 (&#123;&quot;type&quot;=&gt;&quot;es_rejected_execution_exception&quot;, &quot;reason&quot;=&gt;&quot;rejected execution of org.elasticsearch.transport.TransportService$6@6918cf2e on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@55337655[Running, pool size = 24, active threads = 24, queued tasks = 50, completed tasks = 1767887463]]&quot;&#125;)[2017-03-18T09:46:21,043][ERROR][logstash.outputs.elasticsearch] Retrying individual actions 查询官网，确认为时ES的写入遇到了瓶颈1Make sure to watch for TOO_MANY_REQUESTS (429) response codes (EsRejectedExecutionException with the Java client), which is the way that Elasticsearch tells you that it cannot keep up with the current indexing rate. When it happens, you should pause indexing a bit before trying again, ideally with randomized exponential backoff. 我们首先想到的是来调整ES的线程数，但是官网写到”Don’t Touch There Settings!”, 那怎么办？于是乎官方建议我们修改logstash的参数pipeline.batch.size 在ES5.0以后，es将bulk、flush、get、index、search等线程池完全分离，自身的写入不会影响其他功能的性能。来查询一下ES当前的线程情况：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647GET _nodes/stats/thread_pool?pretty&#123; &quot;_nodes&quot;: &#123; &quot;total&quot;: 6, &quot;successful&quot;: 6, &quot;failed&quot;: 0 &#125;, &quot;cluster_name&quot;: &quot;dev-elasticstack5.0&quot;, &quot;nodes&quot;: &#123; &quot;nnfCv8FrSh-p223gsbJVMA&quot;: &#123; &quot;timestamp&quot;: 1489804973926, &quot;name&quot;: &quot;node-3&quot;, &quot;transport_address&quot;: &quot;192.168.3.***:9301&quot;, &quot;host&quot;: &quot;192.168.3.***&quot;, &quot;ip&quot;: &quot;192.168.3.***:9301&quot;, &quot;roles&quot;: [ &quot;master&quot;, &quot;data&quot;, &quot;ingest&quot; ], &quot;attributes&quot;: &#123; &quot;rack&quot;: &quot;r1&quot; &#125;, &quot;thread_pool&quot;: &#123; &quot;bulk&quot;: &#123; &quot;threads&quot;: 24, &quot;queue&quot;: 214, &quot;active&quot;: 24, &quot;rejected&quot;: 30804543, &quot;largest&quot;: 24, &quot;completed&quot;: 1047606679 &#125;, ...... &quot;watcher&quot;: &#123; &quot;threads&quot;: 0, &quot;queue&quot;: 0, &quot;active&quot;: 0, &quot;rejected&quot;: 0, &quot;largest&quot;: 0, &quot;completed&quot;: 0&#125;&#125;&#125;&#125;&#125; 其中：”bulk”模板的线程数24，当前活跃的线程数24，证明所有的线程是busy的状态，queue队列214，rejected为30804543。那么问题就找到了，所有的线程都在忙，队列堵满后再有进程写入就会被拒绝，而当前拒绝数为30804543。 优化方案问题找到了，如何优化呢。官方的建议是提高每次批处理的数量，调节传输间歇时间。当batch.size增大，es处理的事件数就会变少，写入也就越快了。123456vim /etc/logstash/logstash.yml#pipeline.workers: 24pipeline.output.workers: 24pipeline.batch.size: 10000pipeline.batch.delay: 10 具体的worker/output.workers数量建议等于CPU数，batch.size/batch.delay根据实际的数据量逐渐增大来测试最优值。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka性能调优]]></title>
    <url>%2F2018%2F04%2F13%2Fkafka%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Kafka的配置详尽、复杂，想要进行全面的性能调优需要掌握大量信息，这里只记录一下我在日常工作使用中走过的坑和经验来对kafka集群进行优化常用的几点。1.JVM的优化java相关系统自然离不开JVM的优化。首先想到的肯定是Heap Size的调整。 vim bin/kafka-server-start.sh 调整KAFKA_HEAP_OPTS=”-Xmx16G -Xms16G”的值推荐配置：一般HEAP SIZE的大小不超过主机内存的50%。 2.网络和ios操作线程配置优化：1234# broker处理消息的最大线程数num.network.threads=9# broker处理磁盘IO的线程数num.io.threads=16 推荐配置：num.network.threads主要处理网络io，读写缓冲区数据，基本没有io等待，配置线程数量为cpu核数加1。 num.io.threads主要进行磁盘io操作，高峰期可能有些io等待，因此配置需要大些。配置线程数量为cpu核数2倍，最大不超过3倍。 3.socket server可接受数据大小(防止OOM异常)：socket.request.max.bytes=2147483600 推荐配置： 根据自己业务数据包的大小适当调大。这里取值是int类型的，而受限于java int类型的取值范围又不能太大： java int的取值范围为（-2147483648~2147483647），占用4个字节（-2的31次方到2的31次方-1，不能超出，超出之后报错：org.apache.kafka.common.config.ConfigException: Invalid value 8589934592 for configuration socket.request.max.bytes: Not a number of type INT。 4.log数据文件刷盘策略—每当producer写入10000条消息时，刷数据到磁盘—log.flush.interval.messages=10000 —每间隔1秒钟时间，刷数据到磁盘—log.flush.interval.ms=1000 推荐配置： 为了大幅度提高producer写入吞吐量，需要定期批量写文件。一般无需改动，如果topic的数据量较小可以考虑减少log.flush.interval.ms和log.flush.interval.messages来强制刷写数据，减少可能由于缓存数据未写盘带来的不一致。推荐配置分别message 10000，间隔1s。 5.日志保留策略配置—日志保留时长—log.retention.hours=72 —段文件配置—log.segment.bytes=1073741824 推荐配置： 日志建议保留三天，也可以更短；段文件配置1GB，有利于快速回收磁盘空间，重启kafka加载也会加快（kafka启动时是单线程扫描目录(log.dir)下所有数据文件）。如果文件过小，则文件数量比较多。 6.replica复制配置123num.replica.fetchers=3replica.fetch.min.bytes=1replica.fetch.max.bytes=5242880 推荐配置： 每个follow从leader拉取消息进行同步数据，follow同步性能由这几个参数决定，分别为: 拉取线程数(num.replica.fetchers):fetcher配置多可以提高follower的I/O并发度，单位时间内leader持有更多请求，相应负载会增大，需要根据机器硬件资源做权衡，建议适当调大； 最小字节数(replica.fetch.min.bytes):一般无需更改，默认值即可； 最大字节数(replica.fetch.max.bytes)：默认为1MB，这个值太小，推荐5M，根据业务情况调整 最大等待时间(replica.fetch.wait.max.ms):follow拉取频率，频率过高，leader会积压大量无效请求情况，无法进行数据同步，导致cpu飙升。配置时谨慎使用，建议默认值，无需配置。 7.分区数量配置num.partitions=5 推荐配置： 默认partition数量1，如果topic在创建时没有指定partition数量，默认使用此值。Partition的数量选取也会直接影响到Kafka集群的吞吐性能，配置过小会影响消费性能，建议改为5。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 索引查询使用指南]]></title>
    <url>%2F2018%2F04%2F03%2FELK%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[1.我们通常用用_cat API检测集群是否健康。 确保9200端口号可用:curl ‘localhost:9200/_cat/health?v’ 绿色表示一切正常, 黄色表示所有的数据可用但是部分副本还没有分配,红色表示部分数据因为某些原因不可用. 2.通过如下语句，我们可以获取集群的节点列表：curl ‘localhost:9200/_cat/nodes?v’ 3.通过如下语句，列出所有索引：curl ‘localhost:9200/_cat/indices?v’返回结果： 4.创建索引现在我们创建一个名为“customer”的索引，然后再查看所有的索引： curl -XPUT ‘localhost:9200/customer?pretty’ curl ‘localhost:9200/_cat/indices?v’ 结果如下： 上图中红框所表示的是：我们有一个叫customer的索引，它有五个私有的分片以及一个副本，在它里面有0个文档。 5.插入和获取现在我么插入一些数据到集群索引。我们必须给ES指定所以的类型。如下语句：”external” type, ID：1:主体为JSON格式的语句： { “name”: “John Doe” }1234curl -XPUT &apos;localhost:9200/customer/external/1?pretty&apos; -d &apos;&#123; &quot;name&quot;: &quot;John Doe&quot;&#125;&apos; 返回结果为：create：true 表示插入成功。 获取GET，语句如下：1curl -XGET &apos;localhost:9200/customer/external/1?pretty&apos; 其中含义为：获取customer索引下类型为external，id为1的数据，pretty参数表示返回结果格式美观。 6.删除索引 DELETE12curl -XDELETE &apos;localhost:9200/customer?pretty&apos;curl &apos;localhost:9200/_cat/indices?v&apos; 表示索引删除成功。 7.通过以上命令语句的学习，我们发现索引的增删改查有一个类似的格式，总结如下：1234567curl -X&lt;REST Verb&gt; &lt;Node&gt;:&lt;Port&gt;/&lt;Index&gt;/&lt;Type&gt;/&lt;ID&gt;&lt;REST Verb&gt;：REST风格的语法谓词&lt;Node&gt;:节点ip&lt;port&gt;:节点端口号，默认9200&lt;Index&gt;:索引名&lt;Type&gt;:索引类型&lt;ID&gt;:操作对象的ID号 8 修改数据12345678curl -XPUT &apos;localhost:9200/customer/external/1?pretty&apos; -d &apos;&#123; &quot;name&quot;: &quot;John Doe&quot;&#125;&apos;curl -XPUT &apos;localhost:9200/customer/external/1?pretty&apos; -d &apos;&#123; &quot;name&quot;: &quot;Jane Doe&quot;&#125;&apos; 上述命令语句是：先新增id为1，name为John Doe的数据，然后将id为1的name修改为Jane Doe。 9.更新数据9.1 这个例子展示如何将id为1文档的name字段更新为Jane Doe：1234curl -XPOST &apos;localhost:9200/customer/external/1/_update?pretty&apos; -d &apos;&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot; &#125;&#125;&apos; 9.2 这个例子展示如何将id为1数据的name字段更新为Jane Doe同时增加字段age为20:1234curl -XPOST &apos;localhost:9200/customer/external/1/_update?pretty&apos; -d &apos;&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;Jane Doe&quot;, &quot;age&quot;: 20 &#125;&#125;&apos; 9.3 也可以通过一些简单的scripts来执行更新。一下语句通过使用script将年龄增加5:1234curl -XPOST &apos;localhost:9200/customer/external/1/_update?pretty&apos; -d &apos;&#123; &quot;script&quot; : &quot;ctx._source.age += 5&quot;&#125;&apos; 10 删除数据删除数据那是相当的直接. 下面的语句将执行删除Customer中ID为2的数据：1curl -XDELETE &apos;localhost:9200/customer/external/2?pretty&apos; 11 批处理举例:下面语句将在一个批量操作中执行创建索引：123456curl -XPOST &apos;localhost:9200/customer/external/_bulk?pretty&apos; -d &apos;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;name&quot;: &quot;John Doe&quot; &#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125;&#123;&quot;name&quot;: &quot;Jane Doe&quot; &#125;&apos; 下面语句批处理执行更新id为1的数据然后执行删除id为2的数据12345curl -XPOST &apos;localhost:9200/customer/external/_bulk?pretty&apos; -d &apos;&#123;&quot;update&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;doc&quot;: &#123; &quot;name&quot;: &quot;John Doe becomes Jane Doe&quot; &#125; &#125;&#123;&quot;delete&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125;&apos; 12.导入数据集你可以点击这里下载示例数据集:accounts.json其中每个数据都是如下格式:12345678910111213141516&#123; &quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123; &quot;account_number&quot;: 0, &quot;balance&quot;: 16623, &quot;firstname&quot;: &quot;Bradshaw&quot;, &quot;lastname&quot;: &quot;Mckenzie&quot;, &quot;age&quot;: 29, &quot;gender&quot;: &quot;F&quot;, &quot;address&quot;: &quot;244 Columbus Place&quot;, &quot;employer&quot;: &quot;Euron&quot;, &quot;email&quot;: &quot;bradshawmckenzie@euron.com&quot;, &quot;city&quot;: &quot;Hobucken&quot;, &quot;state&quot;: &quot;CO&quot;&#125; 导入示例数据集:12curl -XPOST &apos;localhost:9200/bank/account/_bulk?pretty&apos; --data-binary &quot;@accounts.json&quot;curl &apos;localhost:9200/_cat/indices?v&apos; 上图红框表示我们已经成功批量导入1000条数据索引到bank索引中。 13.查询Sample:12345678910111213141516171819202122232425curl &apos;localhost:9200/bank/_search?q=*&amp;pretty&apos;&#123; &quot;took&quot; : 63, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 1000, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;bank&quot;, &quot;_type&quot; : &quot;account&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123;&quot;account_number&quot;:1,&quot;balance&quot;:39225,&quot;firstname&quot;:&quot;Amber&quot;,&quot;lastname&quot;:&quot;Duke&quot;,&quot;age&quot;:32,&quot;gender&quot;:&quot;M&quot;,&quot;address&quot;:&quot;880 Holmes Lane&quot;,&quot;employer&quot;:&quot;Pyrami&quot;,&quot;email&quot;:&quot;amberduke@pyrami.com&quot;,&quot;city&quot;:&quot;Brogan&quot;,&quot;state&quot;:&quot;IL&quot;&#125; &#125;, &#123; &quot;_index&quot; : &quot;bank&quot;, &quot;_type&quot; : &quot;account&quot;, &quot;_id&quot; : &quot;6&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123;&quot;account_number&quot;:6,&quot;balance&quot;:5686,&quot;firstname&quot;:&quot;Hattie&quot;,&quot;lastname&quot;:&quot;Bond&quot;,&quot;age&quot;:36,&quot;gender&quot;:&quot;M&quot;,&quot;address&quot;:&quot;671 Bristol Street&quot;,&quot;employer&quot;:&quot;Netagy&quot;,&quot;email&quot;:&quot;hattiebond@netagy.com&quot;,&quot;city&quot;:&quot;Dante&quot;,&quot;state&quot;:&quot;TN&quot;&#125; &#125;, &#123; &quot;_index&quot; : &quot;bank&quot;, &quot;_type&quot; : &quot;account&quot;, 上面示例返回所有bank中的索引数据。其中 q=* 表示匹配索引中所有的数据。 等价于:1234curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125;&apos; 14 查询语言 匹配所有数据，但只返回1个:12345curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;size&quot;: 1&#125;&apos; 注意：如果siez不指定，则默认返回10条数据。123456curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 10, &quot;size&quot;: 10&#125;&apos; 返回从11到20的数据。（索引下标从0开始）12345curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: &#123; &quot;balance&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;&#125;&apos; 上述示例匹配所有的索引中的数据，按照balance字段降序排序，并且返回前10条（如果不指定size，默认最多返回10条）。 15.执行搜索 下面例子展示如何返回两个字段（account_number balance）12345curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;_source&quot;: [&quot;account_number&quot;, &quot;balance&quot;]&#125;&apos; 返回account_number 为20 的数据:1234curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;account_number&quot;: 20 &#125; &#125;&#125;&apos; 返回address中包含mill的所有数据：1234curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;&#125;&apos; 返回地址中包含mill或者lane的所有数据：1234curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill lane&quot; &#125; &#125;&#125;&apos; 和上面匹配单个词语不同，下面这个例子是多匹配（match_phrase短语匹配），返回地址中包含短语 “mill lane”的所有数据：1234curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;address&quot;: &quot;mill lane&quot; &#125; &#125;&#125;&apos; 以下是布尔查询，布尔查询允许我们将多个简单的查询组合成一个更复杂的布尔逻辑查询。这个例子将两个查询组合，返回地址中含有mill和lane的所有记录数据：1234567891011curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;lane&quot; &#125; &#125; ] &#125; &#125;&#125;&apos; 上述例子中，must表示所有查询必须都为真才被认为匹配。 相反, 这个例子组合两个查询，返回地址中含有mill或者lane的所有记录数据：1234567891011curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;lane&quot; &#125; &#125; ] &#125; &#125;&#125;&apos; 上述例子中，bool表示查询列表中只要有任何一个为真则认为匹配。 下面例子组合两个查询，返回地址中既没有mill也没有lane的所有数据：1234567891011curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: [ &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;lane&quot; &#125; &#125; ] &#125; &#125;&#125;&apos; 上述例子中,must_not表示查询列表中没有为真的（也就是全为假）时则认为匹配。 我们可以组合must、should、must_not来实现更加复杂的多级逻辑查询。 下面这个例子返回年龄大于40岁、不居住在ID的所有数据：12345678910111213curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;age&quot;: &quot;40&quot; &#125; &#125; ], &quot;must_not&quot;: [ &#123; &quot;match&quot;: &#123; &quot;state&quot;: &quot;ID&quot; &#125; &#125; ] &#125; &#125;&#125;&apos; 16.过滤filter(查询条件设置) 下面这个例子使用了布尔查询返回balance在20000到30000之间的所有数据。12345678910111213141516curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;balance&quot;: &#123; &quot;gte&quot;: 20000, &quot;lte&quot;: 30000 &#125; &#125; &#125; &#125; &#125;&#125;&apos; 17 聚合 Aggregations下面这个例子： 将所有的数据按照state分组（group），然后按照分组记录数从大到小排序，返回前十条（默认）：1234567891011curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_state&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;state&quot; &#125; &#125; &#125;&#125;&apos; 注意：我们设置size=0，不显示查询hits，因为我们只想看返回的聚合结果。 上述语句类似于以下SQL语句：SELECT state, COUNT() FROM bank GROUP BY state ORDER BY COUNT() DESC 下面这个实例按照state分组，降序排序，返回balance的平均值：123456789101112131415161718curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos; -d &apos;&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_state&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;state&quot; &#125;, &quot;aggs&quot;: &#123; &quot;average_balance&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125; &#125;&#125;&apos;]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK的一次吞吐量优化]]></title>
    <url>%2F2018%2F04%2F03%2Felk%2F</url>
    <content type="text"><![CDATA[问题一 ● 最近发现kibana的日志传的很慢，常常查不到日志，由于所有的日志收集都只传输到了一个logstash进行收集和过滤，于是怀疑是否是由于logstash的吞吐量存在瓶颈。一看，还真是到了瓶颈。 ● 优化过程 ● 经过查询logstash完整配置文件，有几个参数需要调整 12345678# pipeline线程数，官方建议是等于CPU内核数pipeline.workers: 24# 实际output时的线程数pipeline.output.workers: 24# 每次发送的事件数pipeline.batch.size: 3000# 发送延时pipeline.batch.delay: 5 PS:由于我们的ES集群数据量较大（&gt;28T），所以具体配置数值视自身生产环境 问题二 ● 在查看logstash日志过程中，我们看到了大量的以下报错12[2017-03-18T09:46:21,043][INFO ][logstash.outputs.elasticsearch] retrying failed action with response code: 429 (&#123;&quot;type&quot;=&gt;&quot;es_rejected_execution_exception&quot;, &quot;reason&quot;=&gt;&quot;rejected execution of org.elasticsearch.transport.TransportService$6@6918cf2e on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@55337655[Running, pool size = 24, active threads = 24, queued tasks = 50, completed tasks = 1767887463]]&quot;&#125;)[2017-03-18T09:46:21,043][ERROR][logstash.outputs.elasticsearch] Retrying individual actions ● 查询官网，确认为时ES的写入遇到了瓶颈 ● Make sure to watch for TOO_MANY_REQUESTS (429) response codes (EsRejectedExecutionException with the Java client), which is the way that Elasticsearch tells you that it cannot keep up with the current indexing rate. When it happens, you should pause indexing a bit before trying again, ideally with randomized exponential backoff. 我们首先想到的是来调整ES的线程数，但是官网写到”Don’t Touch There Settings!”, 那怎么办？于是乎官方建议我们修改logstash的参数pipeline.batch.size ● 在ES5.0以后，es将bulk、flush、get、index、search等线程池完全分离，自身的写入不会影响其他功能的性能。来查询一下ES当前的线程情况：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849GET _nodes/stats/thread_pool?pretty可以看到：&#123; &quot;_nodes&quot;: &#123; &quot;total&quot;: 6, &quot;successful&quot;: 6, &quot;failed&quot;: 0 &#125;, &quot;cluster_name&quot;: &quot;dev-elasticstack5.0&quot;, &quot;nodes&quot;: &#123; &quot;nnfCv8FrSh-p223gsbJVMA&quot;: &#123; &quot;timestamp&quot;: 1489804973926, &quot;name&quot;: &quot;node-3&quot;, &quot;transport_address&quot;: &quot;192.168.3.***:9301&quot;, &quot;host&quot;: &quot;192.168.3.***&quot;, &quot;ip&quot;: &quot;192.168.3.***:9301&quot;, &quot;roles&quot;: [ &quot;master&quot;, &quot;data&quot;, &quot;ingest&quot; ], &quot;attributes&quot;: &#123; &quot;rack&quot;: &quot;r1&quot; &#125;, &quot;thread_pool&quot;: &#123; &quot;bulk&quot;: &#123; &quot;threads&quot;: 24, &quot;queue&quot;: 214, &quot;active&quot;: 24, &quot;rejected&quot;: 30804543, &quot;largest&quot;: 24, &quot;completed&quot;: 1047606679 &#125;, ...... &quot;watcher&quot;: &#123; &quot;threads&quot;: 0, &quot;queue&quot;: 0, &quot;active&quot;: 0, &quot;rejected&quot;: 0, &quot;largest&quot;: 0, &quot;completed&quot;: 0&#125;&#125;&#125;&#125;&#125; 其中：”bulk”模板的线程数24，当前活跃的线程数24，证明所有的线程是busy的状态，queue队列214，rejected为30804543。那么问题就找到了，所有的线程都在忙，队列堵满后再有进程写入就会被拒绝，而当前拒绝数为30804543。优化方案问题找到了，如何优化呢。官方的建议是提高每次批处理的数量，调节传输间歇时间。当batch.size增大，es处理的事件数就会变少，写入也就愉快了。 123456vim /etc/logstash/logstash.yml#pipeline.workers: 24pipeline.output.workers: 24pipeline.batch.size: 10000pipeline.batch.delay: 10 具体的worker/output.workers数量建议等于CPU数，batch.size/batch.delay根据实际的数据量逐渐增大来测试最优值。]]></content>
  </entry>
  <entry>
    <title><![CDATA[23种非常有用的ElasticSearch查询例子]]></title>
    <url>%2F2018%2F04%2F02%2Fasd%2F</url>
    <content type="text"><![CDATA[一、新建索引为了展示Elasticsearch中不同查询的用法，先在Elasticsearch里面创建了book相关的documents，每本书主要涉及以下字段： title, authors, summary, publish_date(发行日期),publisher以及评论条数。 操作如下：123456789101112curl -XPUT &apos;https://www.iteblog.com:9200/iteblog_book_index&apos; -d &apos;&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;&#125;&apos;curl -XPOST &apos;https://www.iteblog.com:9200/iteblog_book_index/book/_bulk&apos; -d &apos;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [&quot;clinton gormley&quot;, &quot;zachary tong&quot;], &quot;summary&quot; : &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot; : &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;oreilly&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;authors&quot;: [&quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot;], &quot;summary&quot; : &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;publish_date&quot; : &quot;2013-01-24&quot;, &quot;num_reviews&quot;: 12, &quot;publisher&quot;: &quot;manning&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;authors&quot;: [&quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot;], &quot;summary&quot; : &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;publish_date&quot; : &quot;2015-12-03&quot;, &quot;num_reviews&quot;: 18, &quot;publisher&quot;: &quot;manning&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4 &#125;&#125;&#123; &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [&quot;trey grainger&quot;, &quot;timothy potter&quot;], &quot;summary&quot; : &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot; : &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; &#125;&apos; 通过dev tools来模拟则为：http://cos.leiyawu.com/img/elk_index_check_1.png 123456789POST /iteblog_book_index/book/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [&quot;clinton gormley&quot;, &quot;zachary tong&quot;], &quot;summary&quot; : &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot; : &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;oreilly&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;authors&quot;: [&quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot;], &quot;summary&quot; : &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;publish_date&quot; : &quot;2013-01-24&quot;, &quot;num_reviews&quot;: 12, &quot;publisher&quot;: &quot;manning&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;authors&quot;: [&quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot;], &quot;summary&quot; : &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;publish_date&quot; : &quot;2015-12-03&quot;, &quot;num_reviews&quot;: 18, &quot;publisher&quot;: &quot;manning&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4 &#125;&#125;&#123; &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [&quot;trey grainger&quot;, &quot;timothy potter&quot;], &quot;summary&quot; : &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot; : &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; &#125; ES中的查询请求有两种方式，一种是简易版的查询，另外一种是使用JSON完整的请求体，叫做结构化查询（DSL）。由于DSL查询更为直观也更为简易，所以大都使用这种方式。DSL查询是POST过去一个json，由于post的请求是json格式的，所以存在很多灵活性，也有很多形式。 基本匹配查询主要形式：（1）、使用Search Lite API，并将所有的搜索参数都通过URL传递（2）、使用Elasticsearch DSL，其可以通过传递一个JSON请求来获取结果。Curl方式与其类似，只是提交方式不是POST，而是XGET，提交参数与DSL提交一致 二、基本匹配查询(Basic Match Query)1、在所有的字段中搜索带有”guide”的结果：通过dev tools:GET /iteblog_book_index/book/_search?q=guide 通过curl方式：curl -u elastic “http://10.104.37.115:9281/iteblog_book_index/book/_search?pretty&quot; -d ‘{ “query”: { “multi_match” : { “query” : “guide”, “fields” : [“_all”] } }}’ 通过DSL：(POST json方式) 其输出和上面使用/iteblog_book_index/book/_search?q=guide的输出一样。上面的multi_match关键字通常在查询多个fields的时候作为match关键字的简写方式。fields属性指定需要查询的字段，如果我们想查询所有的字段，这时候可以使用_all关键字，正如上面的一样。 如果只是查询summary字段，则为： title的Guide则不会显示。 2、以上两种方式都允许我们指定查询哪些字段。比如，我们想查询title中出现in Action的图书，那么我们可以这么查询： GET /iteblog_book_index/book/_search?q=title:in%20action 然而，DSL方式提供了更加灵活的方式来构建更加复杂的查询（我们将在后面看到），甚至指定你想要的返回结果。下面的例子中，我将指定需要返回结果的数量，开始的偏移量（这在分页的情况下非常有用），需要返回document中的哪些字段以及高亮关键字： curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “match” : { “title” : “in action” } }, “size”: 2, #返回结果的数量 “from”: 0, #开始的偏移量 “_source”: [ “title”, “summary”, “publish_date” ], “highlight”: { “fields” : { “title” : {} } }}’ 三、Multi-field Search正如我们之前所看到的，想在一个搜索中查询多个 document field （比如使用同一个查询关键字同时在title和summary中查询），你可以使用multi_match查询，使用如下：curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “multi_match” : { “query” : “elasticsearch guide”, “fields”: [“title”, “summary”] } }}’ 四、Boosting上面使用同一个搜索请求在多个field中查询，你也许想提高某个field的查询权重。在下面的例子中，我们把summary field的权重调成3，这样就提高了其在结果中的权重，这样把_id=4的文档相关性大大提高了，如下： curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “multi_match” : { “query” : “elasticsearch guide”, “fields”: [“title”, “summary^3”] } }, “_source”: [“title”, “summary”, “publish_date”]}’ 需要注意的是：Boosting不仅仅意味着计算出来的分数(calculated score)直接乘以boost factor，最终的boost value会经过归一化以及其他一些内部的优化 五、Bool Query在查询条件中使用AND/OR/NOT操作符，这就是布尔查询(Bool Query)。布尔查询可以接受一个must参数(等价于AND)，一个must_not参数(等价于NOT)，以及一个should参数(等价于OR)。比如，我想查询title中出现Elasticsearch或者Solr关键字的图书，图书的作者是clinton gormley，但没有radu gheorge，可以这么来查询： curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “bool”: { “must”: { “bool” : { “should”: [ { “match”: { “title”: “Elasticsearch” }}, { “match”: { “title”: “Solr” }} ] } }, “must”: { “match”: { “authors”: “clinton gormely” }}, “must_not”: { “match”: {“authors”: “radu gheorge” }} } }}’ 六、Fuzzy Queries（模糊查询）模糊查询可以在Match和 Multi-Match查询中使用以便解决拼写的错误，模糊度是基于Levenshtein distance计算与原单词的距离。使用如下：curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “multi_match” : { “query” : “comprihensiv guide”, “fields”: [“title”, “summary”], “fuzziness”: “AUTO” } }, “_source”: [“title”, “summary”, “publish_date”], “size”: 1}’ 需要注意：上面我们将fuzziness的值指定为AUTO，其在term的长度大于5的时候相当于指定值为2。然而80%的人拼写错误的编辑距离(edit distance)为1，所有如果你将fuzziness设置为1可能会提高你的搜索性能。 七、Wildcard Query(通配符查询)通配符查询允许我们指定一个模式来匹配，而不需要指定完整的term。?将会匹配一个字符；将会匹配零个或者多个字符。比如我们想查找所有作者名字中以t字符开始的记录，我们可以如下使用：curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “wildcard” : { #wildcard是通配符意思 “authors” : “t“ } }, “_source”: [“title”, “authors”], “highlight”: { “fields” : { “authors” : {} } }}’ 八、Regexp Query(正则表达式查询)ElasticSearch还支持正则表达式查询，此方式提供了比通配符查询更加复杂的模式。比如我们先查找作者名字以t字符开头，中间是若干个a-z之间的字符，并且以字符y结束的记录，可以如下查询： curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “regexp” : { “authors” : “t[a-z]*y” } }, “_source”: [“title”, “authors”], “highlight”: { “fields” : { “authors” : {} } }}’ 九、Match Phrase Query(匹配短语查询)匹配短语查询要求查询字符串中的trems要么都出现Document中、要么trems按照输入顺序依次出现在结果中。在默认情况下，查询输入的trems必须在搜索字符串紧挨着出现，否则将查询不到。不过我们可以指定slop参数，来控制输入的trems之间有多少个单词仍然能够搜索到，如下所示：curl -XGET ‘https://www.iteblog.com:9200/iteblog_book_index/book/_search&#39; -d ‘{ “query”: { “multi_match”: { “query”: “search engine”, “fields”: [ “title”, “summary” ], “type”: “phrase”, “slop”: 3 } }, “_source”: [ “title”, “summary”, “publish_date” ]}’ 从上面的例子可以看出，id为4的document被搜索（summary字段里面精确匹配到了search engine），并且分数比较高；而id为1的document也被搜索到了，虽然其summary中的search和engine单词并不是紧挨着的，但是我们指定了slop属性，所以被搜索到了。如果我们将”slop”: 3条件删除，那么id为1的文档将不会被搜索到，如下： 十、Simple Query String(简单查询字符串)simple_query_string是query_string的另一种版本，其更适合为用户提供一个搜索框中，因为其使用+/|/- 分别替换AND/OR/NOT，如果用输入了错误的查询，其直接忽略这种情况而不是抛出异常。使用如下：(注意是POST)curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search{ “query”: { “simple_query_string” : { “query”: “(saerch~1 algorithm~1) + (grant ingersoll) | (tom morton)”, “fields”: [“_all”, “summary^2”] } }, “_source”: [ “title”, “summary”, “authors” ], “highlight”: { “fields” : { “summary” : {} } }} 十一、Term/Terms Query前面的例子中我们已经介绍了全文搜索(full-text search)，但有时候我们对结构化搜索中能够精确匹配并返回搜索结果更感兴趣。这种情况下我们可以使用term和terms查询。在下面例子中，我们想搜索所有曼宁出版社(Manning Publications)出版的图书： curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search -d ‘{ “query”: { “term” : { “publisher”: “manning” } }, “_source” : [“title”,”publish_date”,”publisher”]}’ 还可以使用terms关键字来指定多个terms，如下： { “query”: { “terms” : { “publisher”: [“oreilly”, “packt”] } }} 十二、Term Query - Sorted词查询结果和其他查询结果一样可以很容易地对其进行排序，而且我们可以对输出结果按照多层进行排序：curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search{ “query”: { “term” : { “publisher”: “manning” } }, “_source” : [“title”,”publish_date”,”publisher”], “sort”: [ { “publish_date”: {“order”:”desc”}}, { “title”: { “order”: “desc” }} ]} 执行提示：Fielddata is disabled on text fields by default. Set fielddata=true on [title] in order to load fielddata in memory by uninverting the inverted index 应该是5.x后对排序，聚合这些操作用单独的数据结构(fielddata)缓存到内存里了，需要单独开启 PUT /iteblog_book_index/_mapping/book{ “properties”: { “title”: { “type”: “text”, “fielddata”: true } }} 再次执行： 十三、Range Query(范围查询)另一种结构化查询就是范围查询。在下面例子中，我们搜索所有发行年份为2015的图书：curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search{ “query”: { “range” : { “publish_date”: { “gte”: “2015-01-01”, “lte”: “2015-12-31” } } }, “_source” : [“title”,”publish_date”,”publisher”]} 十四、Filtered Query(过滤查询)过滤查询允许我们对查询结果进行筛选。比如：我们查询标题和摘要中包含Elasticsearch关键字的图书，但是我们想过滤出评论大于20的结果，可以如下使用： curl POST https://www.iteblog.com:9200/iteblog_book_index/book/_search{ “query”: { “filtered”: { “query” : { “multi_match”: { “query”: “elasticsearch”, “fields”: [“title”,”summary”] } }, “filter”: { “range” : { “num_reviews”: { “gte”: 20 } } } } }, “_source” : [“title”,”summary”,”publisher”, “num_reviews”]}]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo使用进阶]]></title>
    <url>%2F2018%2F03%2F19%2Fhexo%E4%BD%BF%E7%94%A8%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[hexo 一、后端管理插件hexo-admin插件可以直接在网页端创建、编辑markdown文章内容，并将内容发布到_posts里。另外，对我而言，最方便的是可以很方便的给文章加标题、分类、打标签。 参见： An Admin Interface for Hexohexo-admin in github 1.安装 (1)npm install –save hexo-admin (2)hexo server -d (3)open http://localhost:4000/admin/ 2.配置 在_config.yml最后添加类似如下内容： admin: username: myfavoritename password_hash: be121740bf988b2225a313fa1f107ca1 secret: a secret something username：后端登录用户名 password_hash：后端登录用户密码对应的md5 hash值 secret：用于保证cookie安全 3.预览]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo fs.SyncWriteStream is deprecated]]></title>
    <url>%2F2018%2F02%2F28%2Fhexo-fs-SyncWriteStream-is-deprecated%2F</url>
    <content type="text"><![CDATA[fs.SyncWriteStream is deprecated出现这个错误需要更新hexo-fs插件 使用npm install hexo-fs –save 在执行hexo命令的时候，总会显示如下报错：(node:7048) [DEP0061] DeprecationWarning: fs.SyncWriteStream is deprecated. 从报错信息来看是因为fs.SyncWriteStream is deprecated，node.js从8.0开始已经弃用了fs.SyncWriteStream方法，所以是因为我们node_modules中某个插件调用了这个方法，通过查看Hexo作者GitHub对应的项目，在issue中看到有人提到这个问题，在hexo项目中其中有一个hexo-fs的插件调用了这个方法，所以需要更新hexo-fs插件，更新方法如下： npm install hexo-fs –save 更新插件后问题依然无法解决。 通过–debug来查看：[root@server init]# hexo –debug06:55:32.711 DEBUG Hexo version: 3.5.006:55:32.714 DEBUG Working directory: /data/wwwroot/init/06:55:32.787 DEBUG Config loaded: /data/wwwroot/init/_config.yml06:55:32.832 DEBUG Plugin loaded: hexo-admin(node:25414) [DEP0061] DeprecationWarning: fs.SyncWriteStream is deprecated. 问题出在：hexo-admin的hexo-fs因hexo-admin作为后台管理，无法npm uninstall hexo-admin卸载,则找到对应文件，注释： [root@server init]# grep -irn “SyncWriteStream” ./node_modules/hexo-admin/./node_modules/hexo-admin/node_modules/hexo-fs/lib/fs.js:718:exports.SyncWriteStream = fs.SyncWriteStream;[root@lywserver init]# 将对应的exports.SyncWriteStream = fs.SyncWriteStream;注释(前面 //)即可！]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>SyncWriteStream </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK x-pack 详细说明]]></title>
    <url>%2F2018%2F02%2F26%2Fname%2F</url>
    <content type="text"><![CDATA[我还能说什么呢？？]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>x-pack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
